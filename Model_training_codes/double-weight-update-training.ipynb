{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install ultralytics\nimport ultralytics\nultralytics.checks()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:40:31.184036Z","iopub.execute_input":"2024-02-01T08:40:31.184396Z","iopub.status.idle":"2024-02-01T08:40:54.339618Z","shell.execute_reply.started":"2024-02-01T08:40:31.184368Z","shell.execute_reply":"2024-02-01T08:40:54.338717Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nSetup complete âœ… (4 CPUs, 31.4 GB RAM, 5432.0/8062.4 GB disk)\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\n\ndevice_lib.list_local_devices()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:41:08.443042Z","iopub.execute_input":"2024-02-01T08:41:08.443568Z","iopub.status.idle":"2024-02-01T08:41:19.554638Z","shell.execute_reply.started":"2024-02-01T08:41:08.443531Z","shell.execute_reply":"2024-02-01T08:41:19.553708Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-02-01 08:41:10.177343: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-01 08:41:10.177451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-01 08:41:10.317703: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 1834663038000405568\n xla_global_id: -1,\n name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 16274030592\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 15760737248013990212\n physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n xla_global_id: 416903419]"},"metadata":{}}]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO('yolov8n.yaml')  # build a new model from scratch\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n\n# Use the model\nresults = model.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name='1Feb2024_model', optimizer = 'Adam')  # train the model\n#results = model.val()  # evaluate model performance on the validation set","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:44:14.970937Z","iopub.execute_input":"2024-02-01T08:44:14.972016Z","iopub.status.idle":"2024-02-01T08:52:38.602157Z","shell.execute_reply.started":"2024-02-01T08:44:14.971981Z","shell.execute_reply":"2024-02-01T08:52:38.600968Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 165MB/s]","output_type":"stream"},{"name":"stdout","text":"Ultralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_model, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_model\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 29.2MB/s]\n2024-02-01 08:44:19,260\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n2024-02-01 08:44:20,012\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=23\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_model', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:07<00:00, 383.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 425.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\nPlotting labels to RGB_Object_detection/1Feb2024_model/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_model\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10       2.5G      2.171      2.366      1.625         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:48<00:00,  3.52it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.462     0.0566     0.0689     0.0303\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.48G      2.105      1.895      1.643         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.233      0.225      0.115     0.0477\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10       2.5G      2.029       1.75      1.604         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.24it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.286      0.163      0.115     0.0494\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.53G      1.929      1.633      1.549         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.426      0.142      0.133     0.0576\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.57G      1.861      1.521      1.502         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.21it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.382      0.202      0.181     0.0831\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.55G      1.796      1.432      1.465         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.527       0.25      0.213        0.1\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.57G      1.749      1.369      1.433         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.05it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.418      0.246      0.228      0.104\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.57G      1.693      1.311      1.398         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.57       0.22      0.231      0.109\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.52G      1.638      1.239      1.356         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.622      0.248      0.261      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.49G      1.594      1.185       1.33         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.506      0.276      0.274      0.135\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.129 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_model/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_model/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_model/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.506      0.276      0.274      0.135\nhuman.pedestrian.adult        300         95      0.614      0.302       0.34       0.17\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.456       0.04      0.045      0.017\n           vehicle.car        300        393      0.654       0.49       0.59      0.306\n         vehicle.truck        300         63      0.279      0.524      0.236      0.142\nmovable_object.barrier        300         80      0.361        0.2      0.189      0.081\n movable_object.debris        300          7          1          0     0.0267    0.00626\nmovable_object.pushable_pullable        300        655      0.722      0.443       0.55      0.262\nmovable_object.trafficcone        300        148      0.466      0.486      0.484      0.227\nSpeed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 1.3ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_model\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO('yolov8n.yaml')  # build a new model from scratch\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n\n# Use the model\nresults = model.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name='1Feb2024_model', optimizer = 'Adam')  # train the model\n#results = model.val()  # evaluate model performance on the validation set","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:53:55.919355Z","iopub.execute_input":"2024-02-01T08:53:55.920030Z","iopub.status.idle":"2024-02-01T09:02:08.869797Z","shell.execute_reply.started":"2024-02-01T08:53:55.919988Z","shell.execute_reply":"2024-02-01T09:02:08.868511Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_model, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_model\nOverriding model.yaml nc=80 with nc=23\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_model', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:06<00:00, 415.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 365.07it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_model/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_model\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.59G      2.403      2.773       1.75        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.178      0.057      0.024    0.00985\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.59G      2.266       2.18      1.717         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.24it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.501     0.0889     0.0671     0.0314\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.53G       2.18      2.041      1.648        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.209      0.131     0.0676     0.0328\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10       2.6G      2.079      1.881      1.588        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.02it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.264      0.142     0.0906     0.0425\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.53G      1.996      1.758      1.547        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.451      0.109     0.0702     0.0329\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.51G      1.915      1.643      1.497        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.92it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.459      0.169      0.134     0.0673\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10       2.6G      1.852      1.543      1.453        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.445      0.177      0.137     0.0698\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.59G      1.785      1.481       1.42        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.98it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.442      0.183       0.13     0.0625\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.58G      1.727       1.38      1.372         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.596      0.158      0.126     0.0638\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.59G       1.67      1.318      1.343        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.11it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.517      0.214      0.175     0.0862\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.129 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_model/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_model/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_model/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.515      0.214      0.175     0.0862\nhuman.pedestrian.adult        300         95      0.264      0.147     0.0919     0.0339\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25          1          0   0.000698   0.000208\n           vehicle.car        300        393      0.637      0.506      0.566      0.297\n         vehicle.truck        300         63      0.251      0.659      0.247      0.152\nmovable_object.barrier        300         80      0.344     0.0875     0.0609     0.0272\n movable_object.debris        300          7          1          0          0          0\nmovable_object.pushable_pullable        300        655      0.516      0.319      0.358      0.153\nmovable_object.trafficcone        300        148      0.627      0.203      0.251      0.113\nSpeed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 1.5ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_model\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_model/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_model/weights/last.pt'\nfused_weights_backup = 'epoch_10_fused.pt'\nfolder_name = '1Feb2024_10epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T09:10:24.371059Z","iopub.execute_input":"2024-02-01T09:10:24.372067Z","iopub.status.idle":"2024-02-01T09:10:27.898531Z","shell.execute_reply.started":"2024-02-01T09:10:24.372010Z","shell.execute_reply":"2024-02-01T09:10:27.897205Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_10_fused.pt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFusion of weights done: \u001b[39m\u001b[38;5;124m\"\u001b[39m, fused_weights_backup)\n\u001b[1;32m     25\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m YOLO(grd_last_weights_file)\n\u001b[0;32m---> 26\u001b[0m results_2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_1\u001b[49m\u001b[38;5;241m.\u001b[39mtrain(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, save_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRD_Object_detection\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mfolder_name, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# train the GRD model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model_1 \u001b[38;5;241m=\u001b[39m YOLO(rgb_last_weights_file)\n\u001b[1;32m     29\u001b[0m results_1 \u001b[38;5;241m=\u001b[39m model_1\u001b[38;5;241m.\u001b[39mtrain(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, save_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB_Object_detection\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mfolder_name, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# train the RGB model\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'model_1' is not defined"],"ename":"NameError","evalue":"name 'model_1' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T09:11:30.731337Z","iopub.execute_input":"2024-02-01T09:11:30.732042Z","iopub.status.idle":"2024-02-01T09:28:09.225593Z","shell.execute_reply.started":"2024-02-01T09:11:30.732014Z","shell.execute_reply":"2024-02-01T09:28:09.224447Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_model/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_10epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_10epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_10epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:02<00:00, 1175.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 924.58it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_10epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_10epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.77G      1.945      1.719      1.546        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:45<00:00,  3.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.42      0.119     0.0932      0.046\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10       2.7G      1.881      1.597      1.527         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.552      0.108      0.093     0.0406\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.64G       1.89      1.595      1.531        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.302      0.168      0.133     0.0618\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.71G      1.848      1.513      1.507        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.324      0.182       0.13     0.0583\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.64G      1.819       1.49      1.503        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.91it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.464       0.18      0.159     0.0746\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.62G      1.761      1.412      1.462        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.444      0.209      0.197     0.0974\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.67G       1.71      1.331       1.43        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.86it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.314      0.221      0.208      0.104\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10       2.7G      1.656      1.285      1.405        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.369      0.195      0.188     0.0948\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10       2.7G      1.618      1.226      1.363         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.05it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.552      0.206      0.218      0.109\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.69G      1.573      1.188       1.34        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.405      0.225      0.209      0.106\n\n10 epochs completed in 0.127 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_10epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_10epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_10epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.551      0.206      0.217      0.109\nhuman.pedestrian.adult        300         95      0.508      0.152        0.2      0.109\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25          1          0    0.00371    0.00207\n           vehicle.car        300        393      0.595      0.588      0.599      0.316\n         vehicle.truck        300         63       0.43      0.492       0.42      0.222\nmovable_object.barrier        300         80      0.242      0.075       0.11     0.0438\n movable_object.debris        300          7          1          0          0          0\nmovable_object.pushable_pullable        300        655      0.564      0.287      0.337      0.149\nmovable_object.trafficcone        300        148      0.623      0.264      0.288      0.135\nSpeed: 0.2ms preprocess, 3.4ms inference, 0.0ms loss, 1.6ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_10epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_model/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_10epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_10epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_10epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1139.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 743.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\nPlotting labels to RGB_Object_detection/1Feb2024_10epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_10epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.82G      1.626      1.222      1.359         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.544      0.231       0.23       0.11\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.63G       1.71      1.331      1.414         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.50it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.217      0.168      0.143     0.0711\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.64G      1.731      1.354      1.433         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.336      0.231      0.209     0.0881\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.68G      1.695      1.318      1.412         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.408      0.197       0.23      0.101\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.68G      1.676      1.288      1.405         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.59it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.564      0.253      0.263      0.127\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10       2.7G      1.649      1.238      1.388         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.457      0.247      0.214      0.106\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.68G      1.605      1.197       1.36         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.347      0.291       0.26      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.68G      1.567      1.144      1.333         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.32it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.383      0.283      0.276      0.136\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.67G      1.518      1.091      1.302         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.03it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.498       0.29      0.304      0.155\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.64G      1.492      1.061      1.284         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.426      0.303      0.299       0.15\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.125 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_10epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_10epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_10epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.506      0.287      0.304      0.155\nhuman.pedestrian.adult        300         95      0.761      0.484      0.529      0.296\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25       0.86       0.12      0.119     0.0367\n           vehicle.car        300        393      0.661      0.588      0.626      0.331\n         vehicle.truck        300         63      0.401       0.54      0.411      0.226\nmovable_object.barrier        300         80      0.424      0.125      0.155     0.0839\n movable_object.debris        300          7          0          0   0.000871   0.000418\nmovable_object.pushable_pullable        300        655      0.679      0.452       0.51      0.233\nmovable_object.trafficcone        300        148      0.769      0.277      0.386      0.186\nSpeed: 0.1ms preprocess, 3.1ms inference, 0.0ms loss, 1.2ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_10epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_10epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_10epoch/weights/last.pt'\nfused_weights_backup = 'epoch_20_fused.pt'\nfolder_name = '1Feb2024_20epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T09:30:04.216068Z","iopub.execute_input":"2024-02-01T09:30:04.216486Z","iopub.status.idle":"2024-02-01T09:47:07.808780Z","shell.execute_reply.started":"2024-02-01T09:30:04.216453Z","shell.execute_reply":"2024-02-01T09:47:07.807644Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_20_fused.pt\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_10epoch/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_20epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_20epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_20epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:02<00:00, 1093.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 936.43it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_20epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_20epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.75G      1.874      1.597      1.511        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.43it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.391      0.214      0.184      0.093\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.74G      1.785      1.438      1.472         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.86it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.325      0.175      0.127      0.062\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.68G        1.8      1.478      1.485        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.53it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.305      0.171      0.125     0.0555\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.76G      1.755      1.403      1.451        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.52it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.553      0.189      0.163     0.0757\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.69G      1.743       1.38      1.463        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.39it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.592      0.207      0.205      0.107\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.67G      1.698      1.321      1.421        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.34it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.38       0.18      0.158     0.0806\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.75G       1.65      1.255      1.392        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.516      0.231      0.199     0.0996\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.74G      1.607      1.204      1.372        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.426      0.195      0.186     0.0973\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.74G      1.563       1.15      1.331         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.65it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.373      0.207      0.206      0.109\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.74G      1.534      1.125      1.318        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.287      0.252      0.219       0.12\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.127 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_20epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_20epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_20epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.287      0.252      0.219       0.12\nhuman.pedestrian.adult        300         95      0.384      0.337      0.235       0.13\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0492     0.0216    0.00446    0.00131\n           vehicle.car        300        393      0.681      0.511      0.584      0.326\n         vehicle.truck        300         63      0.241      0.698      0.388      0.248\nmovable_object.barrier        300         80      0.111     0.0375      0.048     0.0193\n movable_object.debris        300          7          0          0   0.000774   7.74e-05\nmovable_object.pushable_pullable        300        655      0.525      0.369      0.394      0.186\nmovable_object.trafficcone        300        148      0.588      0.291      0.318      0.165\nSpeed: 0.2ms preprocess, 3.6ms inference, 0.0ms loss, 1.8ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_20epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_10epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_20epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_20epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_20epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 956.04it/s] \n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 846.88it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/1Feb2024_20epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_20epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.87G      1.512      1.085        1.3         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.85it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.453      0.286      0.274      0.129\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.64G      1.581      1.172      1.344         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.26it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.538      0.254      0.262      0.131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.64G      1.604      1.196       1.36         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.519       0.26      0.272      0.129\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.69G      1.593      1.182      1.351         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.345      0.291      0.262      0.127\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.72G      1.564      1.153      1.339         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.569      0.244      0.262       0.13\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.71G      1.565      1.131      1.336         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.339       0.33      0.282       0.14\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.72G      1.535      1.106      1.321         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.17it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.488      0.301      0.301      0.145\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.72G      1.493      1.062      1.296         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.29it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.348      0.288      0.278      0.134\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.67G      1.457      1.022       1.27         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.391      0.323      0.318      0.161\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.64G      1.433     0.9906      1.254         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.379      0.319      0.305      0.158\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.126 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_20epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_20epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_20epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.39      0.324      0.318      0.161\nhuman.pedestrian.adult        300         95      0.759      0.496      0.526      0.298\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.191       0.08     0.0335     0.0123\n           vehicle.car        300        393      0.666      0.572      0.623       0.32\n         vehicle.truck        300         63      0.341      0.603      0.416      0.236\nmovable_object.barrier        300         80       0.33        0.2      0.215     0.0933\n movable_object.debris        300          7          0          0     0.0372    0.00752\nmovable_object.pushable_pullable        300        655      0.575      0.545      0.555      0.258\nmovable_object.trafficcone        300        148      0.649      0.419      0.459      0.226\nSpeed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 1.2ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_20epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_20epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_20epoch/weights/last.pt'\nfused_weights_backup = 'epoch_30_fused.pt'\nfolder_name = '1Feb2024_30epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T09:47:50.719802Z","iopub.execute_input":"2024-02-01T09:47:50.720774Z","iopub.status.idle":"2024-02-01T10:05:20.488861Z","shell.execute_reply.started":"2024-02-01T09:47:50.720739Z","shell.execute_reply":"2024-02-01T10:05:20.487719Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_30_fused.pt\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_20epoch/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_30epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_30epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_30epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:02<00:00, 993.99it/s] ","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 814.62it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_30epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_30epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.75G      1.844      1.543      1.493        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:45<00:00,  3.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.236      0.184      0.162     0.0825\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.75G      1.718      1.356      1.432         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.34       0.18      0.142     0.0696\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.68G      1.727      1.356      1.437        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.97it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.339      0.126      0.131     0.0597\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.76G      1.717      1.336      1.429        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.257      0.198      0.156     0.0697\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.69G      1.687      1.314      1.424        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.16it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.507      0.184      0.176     0.0891\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.67G      1.639      1.244      1.384        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.478      0.184      0.174     0.0889\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.76G      1.601        1.2      1.363        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.459      0.222      0.214      0.115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.75G      1.566      1.155      1.349        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.268      0.206      0.179     0.0922\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.74G      1.524        1.1      1.307         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.30it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.426      0.223      0.216      0.115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.74G      1.489      1.071       1.29        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.82it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.404      0.253      0.235      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.127 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_30epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_30epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_30epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.405      0.254      0.235      0.125\nhuman.pedestrian.adult        300         95      0.438      0.295      0.259      0.132\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0595       0.04    0.00732    0.00276\n           vehicle.car        300        393      0.585      0.538      0.597       0.33\n         vehicle.truck        300         63       0.28      0.651      0.457      0.294\nmovable_object.barrier        300         80      0.351      0.075     0.0944     0.0355\n movable_object.debris        300          7          1          0          0          0\nmovable_object.pushable_pullable        300        655      0.459        0.4      0.397      0.185\nmovable_object.trafficcone        300        148       0.47      0.284      0.303       0.15\nSpeed: 0.1ms preprocess, 2.5ms inference, 0.0ms loss, 4.6ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_30epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_20epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_30epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_30epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_30epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:04<00:00, 587.81it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 564.96it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/1Feb2024_30epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_30epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.88G      1.446      1.015      1.269         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.497      0.271      0.268      0.128\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.65G        1.5      1.072      1.296         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.13it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.552      0.275      0.275      0.144\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.66G      1.532      1.109      1.316         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.458      0.238      0.239      0.117\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10       2.7G      1.538      1.116       1.32         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.487      0.235      0.252      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.73G      1.511      1.086      1.309         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.19it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.41      0.293      0.291      0.142\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.72G      1.507      1.073      1.306         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.441      0.291      0.302      0.152\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.73G      1.472      1.038      1.288         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.376      0.288      0.297      0.145\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.74G      1.447      1.006      1.267         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.503      0.308      0.298      0.142\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.68G      1.406     0.9609      1.244         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.577      0.298      0.324      0.169\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.66G      1.385     0.9393      1.228         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.521      0.317      0.315      0.166\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.126 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_30epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_30epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_30epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.579      0.298      0.324      0.169\nhuman.pedestrian.adult        300         95      0.637       0.48      0.497      0.278\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.214       0.08     0.0583     0.0266\n           vehicle.car        300        393      0.819      0.517      0.655      0.358\n         vehicle.truck        300         63      0.399      0.619      0.515      0.318\nmovable_object.barrier        300         80      0.575      0.138      0.191     0.0806\n movable_object.debris        300          7          1          0    0.00321    0.00101\nmovable_object.pushable_pullable        300        655      0.702      0.475      0.542      0.253\nmovable_object.trafficcone        300        148      0.865      0.372      0.451      0.202\nSpeed: 0.3ms preprocess, 2.5ms inference, 0.0ms loss, 3.6ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_30epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_30epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_30epoch/weights/last.pt'\nfused_weights_backup = 'epoch_40_fused.pt'\nfolder_name = '1Feb2024_40epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:19:23.805469Z","iopub.execute_input":"2024-02-01T10:19:23.805871Z","iopub.status.idle":"2024-02-01T10:36:58.186376Z","shell.execute_reply.started":"2024-02-01T10:19:23.805841Z","shell.execute_reply":"2024-02-01T10:36:58.185211Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_40_fused.pt\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_30epoch/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_40epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_40epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_40epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:05<00:00, 517.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 706.68it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_40epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_40epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.83G      1.825      1.531      1.488        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:45<00:00,  3.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.409      0.196      0.174      0.089\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.75G       1.69      1.319      1.408         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.218      0.192      0.151      0.077\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.68G      1.702      1.328      1.419        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.362      0.142      0.123     0.0587\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.76G      1.687      1.296      1.405        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.65it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.232      0.199       0.16      0.077\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.69G       1.65      1.271      1.403        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.478      0.161      0.151     0.0794\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.67G       1.62       1.22      1.374        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.14it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.416      0.182      0.169     0.0864\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.76G      1.577      1.172      1.345        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.04it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.299      0.227      0.207      0.106\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.75G      1.535      1.121      1.329        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.467      0.201      0.205      0.107\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.74G      1.498      1.067      1.288         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.414      0.242      0.219      0.116\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.74G       1.46      1.036      1.272        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.416      0.241      0.254      0.137\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.127 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_40epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_40epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_40epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.416       0.24      0.254      0.137\nhuman.pedestrian.adult        300         95      0.742      0.212      0.282       0.15\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.448       0.04     0.0371     0.0252\n           vehicle.car        300        393      0.776      0.486      0.627      0.347\n         vehicle.truck        300         63      0.374      0.683      0.521      0.343\nmovable_object.barrier        300         80      0.342     0.0625      0.107     0.0385\n movable_object.debris        300          7          0          0   0.000336   0.000134\nmovable_object.pushable_pullable        300        655       0.49      0.376      0.398      0.186\nmovable_object.trafficcone        300        148      0.573      0.304      0.309      0.145\nSpeed: 0.2ms preprocess, 1.5ms inference, 0.0ms loss, 4.4ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_40epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_30epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_40epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_40epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_40epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:03<00:00, 694.68it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 558.44it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/1Feb2024_40epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_40epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.87G      1.403      0.967      1.245         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.00it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.457       0.31      0.279      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.65G      1.448      1.012      1.268         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.379      0.276       0.27      0.141\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.66G      1.485      1.057      1.287         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.19it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.448      0.274      0.252      0.112\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10       2.7G      1.485      1.061      1.292         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.30it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.537      0.272      0.298      0.148\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.73G      1.471      1.048      1.285         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.437      0.294      0.301      0.154\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.72G       1.47       1.02      1.282         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.553      0.278      0.317      0.158\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.73G      1.431      0.987      1.263         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.466      0.318      0.323      0.157\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.73G      1.406     0.9602      1.246         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.87it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.513      0.302      0.307      0.156\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.68G      1.373     0.9302      1.226         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.55it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.462      0.301       0.32       0.17\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.65G      1.353      0.906      1.213         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.404      0.302      0.291      0.154\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.126 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_40epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_40epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_40epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.461      0.302       0.32      0.169\nhuman.pedestrian.adult        300         95      0.836      0.474      0.511      0.283\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.262       0.12     0.0639     0.0345\n           vehicle.car        300        393      0.776      0.511      0.646      0.363\n         vehicle.truck        300         63      0.375      0.667      0.497      0.305\nmovable_object.barrier        300         80      0.322      0.166      0.165     0.0697\n movable_object.debris        300          7          0          0   0.000811   0.000243\nmovable_object.pushable_pullable        300        655      0.752      0.421      0.536      0.253\nmovable_object.trafficcone        300        148      0.823      0.358      0.455      0.217\nSpeed: 0.2ms preprocess, 5.6ms inference, 0.0ms loss, 1.1ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_40epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_40epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_40epoch/weights/last.pt'\nfused_weights_backup = 'epoch_50_fused.pt'\nfolder_name = '1Feb2024_50epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:38:37.673407Z","iopub.execute_input":"2024-02-01T10:38:37.674219Z","iopub.status.idle":"2024-02-01T10:56:08.954009Z","shell.execute_reply.started":"2024-02-01T10:38:37.674184Z","shell.execute_reply":"2024-02-01T10:56:08.952969Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_50_fused.pt\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_40epoch/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_50epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_50epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_50epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:04<00:00, 605.99it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 637.27it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_50epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_50epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.83G       1.82      1.527      1.487        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:45<00:00,  3.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.03it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.373      0.183      0.165     0.0753\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.75G      1.661      1.278      1.393         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.195      0.148      0.122     0.0595\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G      1.679      1.302      1.409        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.96it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.338      0.199      0.165     0.0839\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.77G      1.668      1.278      1.396        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.16it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.47      0.225      0.223      0.108\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10       2.7G      1.625      1.225      1.378        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.87it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.363      0.179      0.149     0.0799\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.68G      1.596      1.186       1.36        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.17it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.417        0.2       0.18     0.0888\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.73G      1.557      1.137      1.332        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.45      0.223      0.215      0.112\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.75G      1.516      1.098      1.315        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.75it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.51      0.193      0.196      0.111\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.75G       1.48      1.045      1.282         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.95it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.473      0.222      0.214      0.116\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.75G      1.446      1.019      1.264        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.554       0.22      0.236      0.131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.128 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_50epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_50epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_50epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.556       0.22      0.236      0.131\nhuman.pedestrian.adult        300         95      0.426      0.137      0.147     0.0816\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.123       0.04     0.0116    0.00612\n           vehicle.car        300        393      0.765      0.455      0.599      0.327\n         vehicle.truck        300         63      0.416      0.698      0.551      0.374\nmovable_object.barrier        300         80          1     0.0724      0.121     0.0445\n movable_object.debris        300          7          1          0          0          0\nmovable_object.pushable_pullable        300        655      0.542      0.385      0.402      0.195\nmovable_object.trafficcone        300        148      0.728      0.189      0.293      0.151\nSpeed: 0.1ms preprocess, 1.4ms inference, 0.0ms loss, 3.4ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_50epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_40epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_50epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_50epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_50epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1029.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 1067.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\nPlotting labels to RGB_Object_detection/1Feb2024_50epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_50epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.87G      1.374     0.9328      1.229         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.21it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.411      0.274      0.286      0.131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.67G      1.418     0.9766      1.251         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.21it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.529       0.27      0.276      0.137\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.67G      1.445       1.01      1.265         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.44      0.256      0.254      0.124\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.72G      1.441      1.012      1.268         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.513      0.233      0.254      0.122\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.71G       1.43      1.001      1.264         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.404      0.268      0.275      0.134\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.73G      1.434     0.9941      1.266         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.06it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.409      0.307      0.297      0.143\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.71G      1.402     0.9619      1.247         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.465      0.318      0.307      0.143\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.72G      1.375      0.932       1.23         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.95it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.388       0.29      0.288       0.14\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10       2.7G      1.347     0.9002      1.211         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.463      0.307      0.328       0.17\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.67G       1.33     0.8839      1.202         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.21it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.462      0.285      0.304      0.161\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.126 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_50epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_50epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_50epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.463      0.307      0.328       0.17\nhuman.pedestrian.adult        300         95      0.765      0.421      0.475      0.253\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.204       0.12     0.0721     0.0304\n           vehicle.car        300        393      0.781      0.545      0.666      0.371\n         vehicle.truck        300         63      0.422      0.556      0.478      0.275\nmovable_object.barrier        300         80      0.442        0.2      0.201     0.0917\n movable_object.debris        300          7          0          0   0.000732   0.000163\nmovable_object.pushable_pullable        300        655      0.693      0.521      0.577      0.276\nmovable_object.trafficcone        300        148      0.857      0.404      0.481      0.233\nSpeed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_50epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_50epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_50epoch/weights/last.pt'\nfused_weights_backup = 'epoch_60_fused.pt'\nfolder_name = '1Feb2024_60epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:58:28.463416Z","iopub.execute_input":"2024-02-01T10:58:28.464449Z","iopub.status.idle":"2024-02-01T11:16:08.588109Z","shell.execute_reply.started":"2024-02-01T10:58:28.464415Z","shell.execute_reply":"2024-02-01T11:16:08.586842Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_60_fused.pt\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_50epoch/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_60epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_60epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_60epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:02<00:00, 1077.54it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 821.16it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_60epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_60epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.76G      1.793      1.484      1.469        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.261     0.0208     0.0156    0.00877\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.75G      1.642      1.253      1.388         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.36      0.186      0.147     0.0684\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.68G      1.651      1.254      1.392        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.95it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.387       0.17      0.161     0.0744\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.76G      1.638      1.228      1.378        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.337      0.218      0.213      0.101\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.69G      1.614      1.215      1.378        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.544      0.173      0.173     0.0907\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.67G      1.571      1.158      1.347        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.50it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.394      0.214      0.189     0.0941\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.76G      1.539      1.116      1.326        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.53it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.324      0.225      0.224      0.113\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.75G      1.501      1.065       1.31        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.34      0.191      0.195      0.104\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.74G      1.463      1.022      1.274         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.44       0.22      0.225      0.124\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.75G      1.422       0.99      1.252        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.367       0.25      0.243      0.133\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.128 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_60epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_60epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_60epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.358      0.252      0.243      0.133\nhuman.pedestrian.adult        300         95       0.43        0.2      0.181     0.0976\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0573       0.04    0.00878    0.00408\n           vehicle.car        300        393      0.726      0.534      0.626      0.361\n         vehicle.truck        300         63       0.42       0.73      0.553      0.369\nmovable_object.barrier        300         80      0.422      0.125      0.134     0.0415\n movable_object.debris        300          7          0          0          0          0\nmovable_object.pushable_pullable        300        655      0.473      0.409      0.406      0.198\nmovable_object.trafficcone        300        148      0.696      0.232      0.275      0.127\nSpeed: 0.1ms preprocess, 1.4ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_60epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_50epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_60epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_60epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_60epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1158.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 989.89it/s] ","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/1Feb2024_60epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_60epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.88G      1.349     0.9111      1.215         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.578      0.259       0.28      0.138\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.67G      1.386     0.9449      1.232         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.12it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.534      0.302      0.312      0.155\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.67G      1.415      0.975      1.249         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.368      0.275      0.268      0.129\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.72G      1.413     0.9778      1.251         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.01it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.391      0.286      0.275      0.134\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.74G      1.414      0.971      1.254         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.425      0.292      0.318      0.155\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.73G      1.397     0.9592      1.247         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.22it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.366       0.29       0.28      0.138\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.74G      1.386     0.9448      1.237         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.432      0.265      0.282      0.137\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.75G      1.348      0.905      1.217         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.418      0.313      0.311      0.156\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.69G      1.323     0.8771      1.196         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.90it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.405      0.291      0.293      0.149\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.67G      1.311     0.8651      1.188         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.65it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.395      0.272      0.286       0.15\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.128 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_60epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_60epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_60epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.414      0.314      0.311      0.156\nhuman.pedestrian.adult        300         95      0.609      0.358       0.37      0.194\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0265       0.08     0.0328     0.0183\n           vehicle.car        300        393      0.687      0.585      0.639       0.35\n         vehicle.truck        300         63      0.281      0.668      0.478      0.308\nmovable_object.barrier        300         80      0.367        0.2      0.188     0.0688\n movable_object.debris        300          7      0.232      0.143      0.146     0.0151\nmovable_object.pushable_pullable        300        655      0.816      0.411      0.531      0.256\nmovable_object.trafficcone        300        148      0.707      0.378      0.414      0.193\nSpeed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 1.2ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_60epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_60epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_60epoch/weights/last.pt'\nfused_weights_backup = 'epoch_70_fused.pt'\nfolder_name = '1Feb2024_70epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T11:21:34.326952Z","iopub.execute_input":"2024-02-01T11:21:34.328033Z","iopub.status.idle":"2024-02-01T11:39:13.863344Z","shell.execute_reply.started":"2024-02-01T11:21:34.327992Z","shell.execute_reply":"2024-02-01T11:39:13.862302Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_70_fused.pt\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_60epoch/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_70epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_70epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_70epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:02<00:00, 1099.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 822.39it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_70epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_70epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.74G      1.781      1.467      1.463        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.398      0.158      0.152     0.0767\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.73G      1.634      1.241      1.385         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.10it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.287      0.196      0.173     0.0859\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.67G       1.65      1.255      1.391        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.75it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.348      0.159      0.146     0.0709\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.75G      1.616      1.205      1.364        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.327      0.246      0.236      0.119\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.68G      1.589      1.187      1.361        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.311      0.211      0.203      0.103\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.66G       1.56      1.145      1.341        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.423      0.191       0.17     0.0886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.75G      1.529      1.096      1.321        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.504      0.197      0.198      0.106\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.73G      1.479      1.051      1.298        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.448      0.216       0.21       0.11\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.73G       1.45      1.009      1.265         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.43it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482        0.4      0.253       0.22      0.119\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.73G      1.409      0.978      1.243        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.12it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.342      0.253      0.233      0.124\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.128 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_70epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_70epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_70epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.343      0.254      0.233      0.124\nhuman.pedestrian.adult        300         95       0.45      0.253      0.193      0.104\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0438       0.04     0.0151    0.00829\n           vehicle.car        300        393      0.725       0.49       0.59       0.33\n         vehicle.truck        300         63      0.285      0.683      0.454       0.28\nmovable_object.barrier        300         80      0.331        0.1      0.103     0.0381\n movable_object.debris        300          7          0          0          0          0\nmovable_object.pushable_pullable        300        655      0.475      0.439      0.428        0.2\nmovable_object.trafficcone        300        148      0.774      0.278      0.316      0.157\nSpeed: 0.1ms preprocess, 1.6ms inference, 0.0ms loss, 1.7ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_70epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_60epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_70epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_70epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_70epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1073.27it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 1037.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\nPlotting labels to RGB_Object_detection/1Feb2024_70epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_70epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.86G      1.326     0.8852      1.204         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:47<00:00,  3.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.529      0.268      0.264      0.124\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.66G      1.357     0.9176      1.216         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.402      0.298      0.274      0.135\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.67G      1.387       0.95      1.235         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.345      0.243      0.252      0.118\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.71G      1.399     0.9678      1.247         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.382       0.27      0.284      0.139\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10       2.7G      1.397     0.9582      1.245         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.24it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.501      0.275      0.284      0.144\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.73G      1.386     0.9472      1.243         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.433      0.297      0.295      0.146\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10       2.7G      1.363     0.9209      1.227         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.09it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.443      0.317      0.317      0.149\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.71G      1.332     0.8903      1.205         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.43      0.285      0.297       0.15\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.69G      1.308     0.8601      1.191         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.455      0.291      0.335      0.178\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.67G      1.291     0.8457       1.18         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.19it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.44      0.286      0.311      0.164\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.128 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_70epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_70epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_70epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.456      0.289      0.335      0.178\nhuman.pedestrian.adult        300         95      0.817      0.368      0.507      0.277\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.189       0.08     0.0857     0.0399\n           vehicle.car        300        393      0.777      0.601       0.68      0.369\n         vehicle.truck        300         63      0.406      0.603      0.556      0.371\nmovable_object.barrier        300         80      0.343      0.188      0.201     0.0861\n movable_object.debris        300          7          0          0          0          0\nmovable_object.pushable_pullable        300        655      0.688      0.461      0.546       0.26\nmovable_object.trafficcone        300        148       0.88      0.297      0.439      0.203\nSpeed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_70epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_70epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_70epoch/weights/last.pt'\nfused_weights_backup = 'epoch_80_fused.pt'\nfolder_name = '1Feb2024_80epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T11:40:26.911878Z","iopub.execute_input":"2024-02-01T11:40:26.912335Z","iopub.status.idle":"2024-02-01T11:58:08.132993Z","shell.execute_reply.started":"2024-02-01T11:40:26.912301Z","shell.execute_reply":"2024-02-01T11:58:08.131699Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_80_fused.pt\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_70epoch/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_80epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_80epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_80epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:04<00:00, 548.06it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 689.09it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_80epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_80epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.83G      1.781      1.477      1.463        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.291       0.19      0.175     0.0889\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.75G      1.628      1.225      1.373         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.239      0.222      0.174     0.0873\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G      1.628      1.219      1.383        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.405       0.19      0.177     0.0885\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.77G      1.609      1.195      1.358        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.329        0.2      0.201      0.092\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10       2.7G      1.584      1.168      1.358        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.82it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.413      0.139      0.115     0.0605\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.68G       1.55      1.125      1.332        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.26it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.431      0.177      0.162     0.0853\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.73G      1.514      1.082      1.307        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.536      0.196      0.213      0.112\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.75G       1.47      1.032      1.289        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.91it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.496      0.203      0.222      0.116\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.75G      1.435     0.9926      1.257         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.476      0.217      0.208      0.112\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.75G      1.397     0.9656      1.237        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.361      0.226      0.225      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.129 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_80epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_80epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_80epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.364      0.225      0.224      0.124\nhuman.pedestrian.adult        300         95      0.471      0.137      0.134     0.0789\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0811       0.04     0.0135    0.00641\n           vehicle.car        300        393      0.762      0.457      0.592      0.341\n         vehicle.truck        300         63      0.309      0.683       0.48      0.312\nmovable_object.barrier        300         80      0.364      0.075      0.104     0.0469\n movable_object.debris        300          7          0          0          0          0\nmovable_object.pushable_pullable        300        655      0.526      0.415      0.424        0.2\nmovable_object.trafficcone        300        148      0.767      0.222      0.273      0.134\nSpeed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 1.5ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_80epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_70epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_80epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_80epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_80epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1100.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 1059.43it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/1Feb2024_80epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_80epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.87G      1.303     0.8659      1.192         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.502      0.276      0.287       0.14\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.67G      1.338     0.9024      1.207         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.479      0.285      0.322      0.166\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.68G      1.371     0.9257      1.226         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.44       0.24      0.239      0.114\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.72G      1.378      0.944      1.236         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.17it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.322      0.262      0.237       0.12\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.75G      1.371     0.9353      1.228         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.469      0.297      0.307       0.15\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.74G      1.366     0.9186      1.226         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.405      0.302       0.29      0.145\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.75G      1.341     0.9017      1.214         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.463      0.298      0.293      0.147\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.76G      1.318     0.8739      1.198         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.34it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.355      0.281      0.259       0.13\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10       2.7G      1.292     0.8493       1.18         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.01it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.448      0.319      0.326      0.169\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.68G      1.276     0.8327      1.171         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.422      0.288      0.288      0.152\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.127 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_80epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_80epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_80epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.448       0.32      0.326      0.169\nhuman.pedestrian.adult        300         95      0.749      0.441      0.466      0.255\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.136       0.12     0.0487     0.0194\n           vehicle.car        300        393      0.807      0.564      0.682      0.379\n         vehicle.truck        300         63      0.337      0.635      0.469      0.304\nmovable_object.barrier        300         80      0.436       0.25      0.251     0.0911\n movable_object.debris        300          7          0          0   0.000242   9.67e-05\nmovable_object.pushable_pullable        300        655      0.699      0.493      0.576      0.281\nmovable_object.trafficcone        300        148      0.869      0.378      0.445      0.193\nSpeed: 0.4ms preprocess, 7.0ms inference, 0.0ms loss, 1.8ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_80epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_80epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_80epoch/weights/last.pt'\nfused_weights_backup = 'epoch_90_fused.pt'\nfolder_name = '1Feb2024_90epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T11:59:30.326205Z","iopub.execute_input":"2024-02-01T11:59:30.326964Z","iopub.status.idle":"2024-02-01T12:17:02.693677Z","shell.execute_reply.started":"2024-02-01T11:59:30.326932Z","shell.execute_reply":"2024-02-01T12:17:02.692628Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_90_fused.pt\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_80epoch/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_90epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_90epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_90epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:02<00:00, 1074.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 848.21it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_90epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_90epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.74G      1.793      1.505      1.472        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.98it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.398      0.158       0.14     0.0708\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.74G      1.615      1.208      1.363         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  4.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.597      0.165      0.152     0.0799\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.68G      1.626      1.222      1.377        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  3.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.331      0.166      0.133      0.063\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.76G      1.612      1.208      1.362        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.11it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.554       0.19      0.201     0.0974\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.69G      1.575      1.166      1.351        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.549      0.173      0.163     0.0824\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.67G      1.541      1.119      1.327        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.356      0.196      0.206      0.108\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.75G       1.51      1.072      1.306        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.73it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.658       0.21      0.221       0.12\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.74G      1.468      1.029      1.288        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.15it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.394       0.18      0.213      0.109\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.74G      1.427     0.9786      1.255         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.14it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.505      0.218      0.223      0.116\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.74G      1.391     0.9509      1.236        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.462      0.263      0.241       0.13\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.129 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_90epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_90epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_90epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.461      0.263      0.241       0.13\nhuman.pedestrian.adult        300         95       0.46      0.242      0.228      0.117\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0431       0.08    0.00976    0.00387\n           vehicle.car        300        393      0.794      0.481       0.62      0.353\n         vehicle.truck        300         63      0.343      0.746      0.456      0.295\nmovable_object.barrier        300         80      0.343      0.113      0.125      0.056\n movable_object.debris        300          7          1          0          0          0\nmovable_object.pushable_pullable        300        655      0.478      0.435      0.415      0.199\nmovable_object.trafficcone        300        148      0.684       0.27      0.315      0.147\nSpeed: 0.1ms preprocess, 2.8ms inference, 0.0ms loss, 3.8ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_90epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_80epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_90epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_90epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_90epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1143.86it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 970.32it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/1Feb2024_90epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_90epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.92G      1.282     0.8482       1.18         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.516      0.284      0.274      0.129\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10       2.7G      1.324     0.8888        1.2         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.475      0.253      0.282      0.143\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G      1.354     0.9199      1.216         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.32      0.287       0.26      0.128\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.73G      1.367     0.9311      1.225         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.94it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.441      0.309      0.297      0.144\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.73G      1.354     0.9116      1.221         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.80it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.429      0.272      0.291      0.142\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.75G      1.349     0.9144      1.219         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.395      0.308       0.29      0.143\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.73G      1.328     0.8898      1.208         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482        0.4      0.295      0.289      0.146\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.73G       1.31     0.8661      1.196         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.546      0.276      0.291      0.142\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.71G      1.276      0.831      1.172         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.466      0.308      0.325      0.163\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.68G      1.258     0.8133      1.162         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.582      0.286       0.32      0.164\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.149 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_90epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_90epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_90epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.456      0.314      0.325      0.163\nhuman.pedestrian.adult        300         95      0.785      0.453      0.515      0.249\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.162       0.16      0.101     0.0434\n           vehicle.car        300        393      0.805      0.527      0.648      0.361\n         vehicle.truck        300         63      0.415      0.587      0.414       0.24\nmovable_object.barrier        300         80      0.375      0.225      0.221      0.094\n movable_object.debris        300          7          0          0   0.000937   0.000207\nmovable_object.pushable_pullable        300        655      0.727      0.508      0.584      0.267\nmovable_object.trafficcone        300        148      0.836      0.365      0.437      0.214\nSpeed: 0.2ms preprocess, 1.4ms inference, 0.0ms loss, 1.9ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_90epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/1Feb2024_90epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/1Feb2024_90epoch/weights/last.pt'\nfused_weights_backup = 'epoch_100_fused.pt'\nfolder_name = '1Feb2024_100epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, grd_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\n\nmodel_2 = YOLO(grd_last_weights_file)\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name=folder_name, optimizer = 'Adam')  # train the GRD model\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the RGB model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T12:17:57.751516Z","iopub.execute_input":"2024-02-01T12:17:57.752217Z","iopub.status.idle":"2024-02-01T12:35:30.997995Z","shell.execute_reply.started":"2024-02-01T12:17:57.752184Z","shell.execute_reply":"2024-02-01T12:35:30.996973Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_100_fused.pt\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/1Feb2024_90epoch/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=1Feb2024_100epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/1Feb2024_100epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/1Feb2024_100epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:04<00:00, 561.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 295.87it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/1Feb2024_100epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/1Feb2024_100epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.74G      1.779      1.474      1.469        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.53      0.156      0.155     0.0774\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.74G      1.603      1.201      1.363         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.91it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.313      0.187      0.172     0.0914\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.68G      1.622      1.206      1.377        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.461       0.18      0.179       0.09\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.76G      1.604      1.178      1.361        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.18it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.418      0.196      0.164     0.0747\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.69G      1.583       1.16      1.362        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.288      0.161      0.158     0.0778\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.67G      1.524      1.107      1.319        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.59it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.59      0.194       0.19        0.1\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.72G      1.496      1.058      1.301        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.558      0.212      0.246       0.13\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.75G      1.452      1.017      1.285        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.468      0.207       0.21      0.109\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.74G       1.42      0.969      1.252         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.476      0.245      0.234      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.74G      1.381      0.938       1.23        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:40<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.59it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.392      0.253      0.244      0.129\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.128 hours.\nOptimizer stripped from GRD_Object_detection/1Feb2024_100epoch/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/1Feb2024_100epoch/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/1Feb2024_100epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.56      0.211      0.246       0.13\nhuman.pedestrian.adult        300         95      0.389      0.358      0.335      0.176\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25          0          0     0.0032    0.00142\n           vehicle.car        300        393      0.749      0.499      0.606      0.326\n         vehicle.truck        300         63      0.456      0.635      0.545      0.338\nmovable_object.barrier        300         80      0.637     0.0875      0.128     0.0516\n movable_object.debris        300          7          1          0          0          0\nmovable_object.pushable_pullable        300        655       0.81      0.169      0.319      0.155\nmovable_object.trafficcone        300        148          1      0.154      0.277      0.123\nSpeed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 1.3ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/1Feb2024_100epoch\u001b[0m\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/1Feb2024_90epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=1Feb2024_100epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/1Feb2024_100epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/1Feb2024_100epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1021.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 808.61it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/1Feb2024_100epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/1Feb2024_100epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.77G       1.27     0.8336      1.175         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.409      0.287      0.286      0.142\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10       2.7G      1.306     0.8723       1.19         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.13it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.534      0.247      0.281      0.143\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G      1.329     0.8947      1.205         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.476      0.265      0.256      0.131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.73G      1.354     0.9196      1.222         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.413      0.276      0.281      0.144\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.72G      1.337      0.896      1.212         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.581      0.267      0.287      0.127\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.75G      1.343     0.8946      1.214         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.468      0.277      0.308       0.14\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.72G      1.311       0.87      1.199         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.352      0.279      0.275      0.141\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.73G      1.293     0.8468      1.186         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:39<00:00,  4.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.392       0.31      0.305      0.155\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.71G      1.257     0.8162      1.165         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.85it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.415       0.33      0.322      0.166\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.69G      1.252     0.8041      1.162         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.53it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.423      0.305      0.308      0.164\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.143 hours.\nOptimizer stripped from RGB_Object_detection/1Feb2024_100epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/1Feb2024_100epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/1Feb2024_100epoch/weights/best.pt...\nUltralytics YOLOv8.1.8 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.414      0.329      0.321      0.166\nhuman.pedestrian.adult        300         95      0.742      0.537      0.523      0.274\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0746       0.08     0.0591     0.0296\n           vehicle.car        300        393      0.778      0.557      0.657      0.358\n         vehicle.truck        300         63        0.3      0.635      0.393      0.241\nmovable_object.barrier        300         80      0.408      0.237       0.23     0.0921\n movable_object.debris        300          7          0          0          0          0\nmovable_object.pushable_pullable        300        655      0.685      0.513      0.588      0.273\nmovable_object.trafficcone        300        148      0.737      0.405      0.443      0.224\nSpeed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/1Feb2024_100epoch\u001b[0m\n","output_type":"stream"}]}]}
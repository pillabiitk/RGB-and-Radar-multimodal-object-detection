{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7435425,"sourceType":"datasetVersion","datasetId":4327201},{"sourceId":7435806,"sourceType":"datasetVersion","datasetId":4327551},{"sourceId":7436632,"sourceType":"datasetVersion","datasetId":4328142}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install ultralytics\nimport ultralytics\nultralytics.checks()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T12:13:42.413380Z","iopub.execute_input":"2024-01-26T12:13:42.414454Z","iopub.status.idle":"2024-01-26T12:14:02.266321Z","shell.execute_reply.started":"2024-01-26T12:13:42.414413Z","shell.execute_reply":"2024-01-26T12:14:02.265377Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nSetup complete âœ… (4 CPUs, 31.4 GB RAM, 5359.4/8062.4 GB disk)\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\n\ndevice_lib.list_local_devices()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T12:15:04.859973Z","iopub.execute_input":"2024-01-26T12:15:04.860933Z","iopub.status.idle":"2024-01-26T12:15:16.489203Z","shell.execute_reply.started":"2024-01-26T12:15:04.860898Z","shell.execute_reply":"2024-01-26T12:15:16.488233Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 9817440673574873754\n xla_global_id: -1,\n name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 16274030592\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 8415274932581741544\n physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n xla_global_id: 416903419]"},"metadata":{}}]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO('yolov8n.yaml')  # build a new model from scratch\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n\n# Use the model\nresults = model.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name='26Jan2024_model', optimizer = 'Adam')  # train the model\nresults = model.val()  # evaluate model performance on the validation set","metadata":{"execution":{"iopub.status.busy":"2024-01-26T12:18:24.042148Z","iopub.execute_input":"2024-01-26T12:18:24.042898Z","iopub.status.idle":"2024-01-26T12:27:15.357924Z","shell.execute_reply.started":"2024-01-26T12:18:24.042865Z","shell.execute_reply":"2024-01-26T12:27:15.356684Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 110MB/s]","output_type":"stream"},{"name":"stdout","text":"Ultralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_model, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_model\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 19.7MB/s]\n2024-01-26 12:18:27,961\tINFO util.py:129 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n2024-01-26 12:18:28,424\tINFO util.py:129 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=23\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_model', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:06<00:00, 392.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 390.17it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/26Jan2024_model/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_model\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10       2.5G      2.167      2.368      1.604         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:48<00:00,  3.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.267      0.151     0.0833     0.0344\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.48G      2.116      1.908      1.601         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.378      0.172       0.11     0.0438\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10       2.5G      2.024      1.749      1.553         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.453      0.182      0.141     0.0634\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.53G      1.938      1.622      1.502         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.479      0.137      0.151     0.0696\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.57G      1.853      1.528       1.46         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.65it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.448      0.141      0.114     0.0565\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.55G      1.798      1.439      1.429         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.97it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.473      0.237      0.192     0.0871\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.57G      1.737      1.372      1.392         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.00it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.387      0.257      0.219      0.104\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.57G      1.683      1.301      1.357         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.554      0.248      0.251      0.127\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.52G      1.628      1.231       1.32         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.92it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.614      0.255       0.26       0.13\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.49G      1.593      1.182      1.302         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.567      0.287       0.28      0.138\n\n10 epochs completed in 0.132 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_model/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_model/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_model/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.568      0.286      0.279      0.138\nhuman.pedestrian.adult        300         95      0.738      0.411      0.466      0.233\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25          1          0     0.0593     0.0134\n           vehicle.car        300        393      0.733      0.517      0.648      0.347\n         vehicle.truck        300         63      0.243      0.587      0.251       0.16\nmovable_object.barrier        300         80      0.356        0.2      0.202     0.0832\n movable_object.debris        300          7          1          0    0.00985    0.00456\nmovable_object.pushable_pullable        300        655      0.574      0.493      0.505      0.233\nmovable_object.trafficcone        300        148      0.467      0.365      0.372       0.17\nSpeed: 0.7ms preprocess, 1.3ms inference, 0.0ms loss, 9.3ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_model\u001b[0m\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 1043.76it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:07<00:00,  2.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.567      0.287      0.279      0.139\nhuman.pedestrian.adult        300         95      0.724      0.411      0.462      0.234\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25          1          0     0.0593      0.013\n           vehicle.car        300        393      0.731      0.518      0.647      0.346\n         vehicle.truck        300         63      0.242      0.587      0.251      0.159\nmovable_object.barrier        300         80      0.361        0.2      0.202     0.0827\n movable_object.debris        300          7          1          0    0.00985    0.00457\nmovable_object.pushable_pullable        300        655      0.577      0.496      0.507      0.234\nmovable_object.trafficcone        300        148      0.466      0.372      0.377      0.174\nSpeed: 0.8ms preprocess, 2.1ms inference, 0.0ms loss, 1.6ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_model2\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO('yolov8n.yaml')  # build a new model from scratch\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n\n# Use the model\nresults = model.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='GRD_Object_detection', name='26Jan2024_model', optimizer = 'Adam')  # train the model\nresults = model.val()  # evaluate model performance on the validation set","metadata":{"execution":{"iopub.status.busy":"2024-01-26T12:29:22.145528Z","iopub.execute_input":"2024-01-26T12:29:22.145966Z","iopub.status.idle":"2024-01-26T12:38:01.303435Z","shell.execute_reply.started":"2024-01-26T12:29:22.145923Z","shell.execute_reply":"2024-01-26T12:38:01.302239Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=26Jan2024_model, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/26Jan2024_model\nOverriding model.yaml nc=80 with nc=23\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/26Jan2024_model', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:07<00:00, 377.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 373.74it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/26Jan2024_model/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/26Jan2024_model\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.62G      2.409      2.784      1.736        158        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.57it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.463     0.0155    0.00713    0.00295\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.62G      2.271      2.193      1.692         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  3.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.02it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482     0.0137      0.206     0.0215    0.00805\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.56G       2.18      2.047      1.639        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  3.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.39it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.313      0.106     0.0562     0.0286\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.63G      2.074      1.868      1.567        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.376      0.125     0.0728     0.0355\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.56G      2.002      1.764      1.537        203        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  4.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.02it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.485      0.165      0.132     0.0618\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.54G      1.922       1.65      1.488        200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.523      0.144      0.105      0.048\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.63G      1.852      1.553      1.442        174        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  4.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.94it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.403      0.171      0.128     0.0606\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.62G      1.784      1.465      1.414        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.19it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.476      0.176      0.138     0.0686\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.62G      1.727      1.385      1.362         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.481       0.18       0.17     0.0835\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.62G      1.673      1.318      1.332        177        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.363      0.211      0.186     0.0933\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.131 hours.\nOptimizer stripped from GRD_Object_detection/26Jan2024_model/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/26Jan2024_model/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/26Jan2024_model/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.363      0.211      0.186     0.0934\nhuman.pedestrian.adult        300         95      0.447      0.137      0.191     0.0836\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25          1          0    0.00191   0.000601\n           vehicle.car        300        393      0.619      0.468      0.534      0.289\n         vehicle.truck        300         63      0.249      0.603      0.312      0.186\nmovable_object.barrier        300         80      0.214      0.125      0.114     0.0451\n movable_object.debris        300          7          0          0          0          0\nmovable_object.pushable_pullable        300        655      0.502      0.292      0.311      0.136\nmovable_object.trafficcone        300        148      0.235       0.27      0.213      0.101\nSpeed: 0.6ms preprocess, 1.3ms inference, 0.0ms loss, 1.1ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/26Jan2024_model\u001b[0m\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 1051.35it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:06<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.365      0.211      0.186     0.0933\nhuman.pedestrian.adult        300         95      0.447      0.137       0.19     0.0822\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25          1          0     0.0019   0.000598\n           vehicle.car        300        393      0.619      0.466      0.532       0.29\n         vehicle.truck        300         63      0.246      0.603      0.312      0.185\nmovable_object.barrier        300         80      0.234      0.134      0.115      0.045\n movable_object.debris        300          7          0          0          0          0\nmovable_object.pushable_pullable        300        655      0.501      0.292       0.31      0.136\nmovable_object.trafficcone        300        148      0.237       0.27      0.217      0.101\nSpeed: 0.7ms preprocess, 4.0ms inference, 0.0ms loss, 1.3ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/26Jan2024_model2\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"#!rm -rf /kaggle/working/epoch_10_fused.pt","metadata":{"execution":{"iopub.status.busy":"2024-01-26T12:48:43.044851Z","iopub.execute_input":"2024-01-26T12:48:43.045600Z","iopub.status.idle":"2024-01-26T12:48:43.050402Z","shell.execute_reply.started":"2024-01-26T12:48:43.045563Z","shell.execute_reply":"2024-01-26T12:48:43.049377Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Load the first model\nmodel1_dict = torch.load('/kaggle/working/RGB_Object_detection/26Jan2024_model/weights/last.pt')\n\n# Load the second model\nmodel2_dict = torch.load('/kaggle/working/GRD_Object_detection/26Jan2024_model/weights/last.pt')\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, '/kaggle/working/RGB_Object_detection/26Jan2024_model/weights/last.pt')\ntorch.save(model1_dict, 'epoch_10_fused.pt')\nprint(\"Fusion of weights done\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T12:49:02.404485Z","iopub.execute_input":"2024-01-26T12:49:02.404879Z","iopub.status.idle":"2024-01-26T12:49:05.023344Z","shell.execute_reply.started":"2024-01-26T12:49:02.404831Z","shell.execute_reply":"2024-01-26T12:49:05.022253Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fusion of weights done\n","output_type":"stream"}]},{"cell_type":"code","source":"#!rm -rf /kaggle/working/RGB_Object_detection/26Jan2024_80epoch","metadata":{"execution":{"iopub.status.busy":"2024-01-26T16:04:46.095140Z","iopub.execute_input":"2024-01-26T16:04:46.095536Z","iopub.status.idle":"2024-01-26T16:04:47.184340Z","shell.execute_reply.started":"2024-01-26T16:04:46.095500Z","shell.execute_reply":"2024-01-26T16:04:47.182969Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model_1 = YOLO('/kaggle/working/RGB_Object_detection/26Jan2024_model/weights/last.pt')\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name='26Jan2024_10epoch', optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T12:59:37.304386Z","iopub.execute_input":"2024-01-26T12:59:37.305106Z","iopub.status.idle":"2024-01-26T13:08:07.849900Z","shell.execute_reply.started":"2024-01-26T12:59:37.305075Z","shell.execute_reply":"2024-01-26T13:08:07.848908Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_model/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_10epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_10epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_10epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1038.67it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 847.17it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/26Jan2024_10epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_10epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.76G       1.63      1.214      1.328         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:47<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.587      0.254      0.247      0.118\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.59G      1.705      1.326      1.372         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.592      0.202      0.216      0.106\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10       2.6G       1.72      1.344      1.391         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.65it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.416      0.281       0.22     0.0989\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.64G      1.716      1.338      1.386         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.451      0.204       0.22      0.103\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.68G      1.669      1.274      1.364         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.416      0.254      0.228      0.106\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.66G      1.645      1.232       1.35         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.465      0.227      0.202     0.0983\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.68G        1.6      1.187      1.323         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.00it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.477      0.304      0.267      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.68G      1.567      1.141      1.299         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.24it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.456       0.27       0.27      0.134\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.63G      1.529      1.098      1.278         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.387       0.27      0.261      0.127\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10       2.6G      1.495      1.059      1.257         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.75it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.364      0.303      0.283      0.143\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.129 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_10epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_10epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_10epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.363      0.303      0.282      0.143\nhuman.pedestrian.adult        300         95      0.764      0.305      0.396      0.207\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.254     0.0821     0.0776     0.0323\n           vehicle.car        300        393       0.73      0.532      0.648      0.355\n         vehicle.truck        300         63      0.265      0.554      0.259      0.168\nmovable_object.barrier        300         80      0.158        0.3      0.146     0.0651\n movable_object.debris        300          7          0          0     0.0202    0.00642\nmovable_object.pushable_pullable        300        655      0.583       0.51       0.54      0.241\nmovable_object.trafficcone        300        148      0.516      0.446      0.455      0.214\nSpeed: 0.5ms preprocess, 1.3ms inference, 0.0ms loss, 1.2ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_10epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"model_2 = YOLO('/kaggle/working/GRD_Object_detection/26Jan2024_model/weights/last.pt')  # load a pretrained model (recommended for training)\n\n# Use the model\nresults_2 = model_2.train(data='/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml', epochs=90, save_period=2, device=0, project='GRD_Object_detection', name='26Jan2024', optimizer = 'Adam')  # train the model\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:16:44.016175Z","iopub.execute_input":"2024-01-26T13:16:44.017032Z","iopub.status.idle":"2024-01-26T14:33:33.067532Z","shell.execute_reply.started":"2024-01-26T13:16:44.016995Z","shell.execute_reply":"2024-01-26T14:33:33.066264Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/GRD_Object_detection/26Jan2024_model/weights/last.pt, data=/kaggle/input/grd-dataset-kaggle/grd_dataset_kaggle_updated.yaml, epochs=90, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=GRD_Object_detection, name=26Jan2024, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=GRD_Object_detection/26Jan2024\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir GRD_Object_detection/26Jan2024', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train/labels... 2700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:02<00:00, 1047.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 613.56it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to GRD_Object_detection/26Jan2024/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mGRD_Object_detection/26Jan2024\u001b[0m\nStarting training for 90 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/90      3.21G        1.8      1.486      1.382        288        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:53<00:00,  3.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.451      0.169      0.148     0.0751\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/90      3.11G      1.808      1.497        1.4        259        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.57it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.539      0.175      0.162     0.0771\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/90      3.52G      1.829       1.53       1.42        179        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.55it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.336      0.158      0.123     0.0511\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/90      3.79G      1.805      1.491      1.409        310        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:49<00:00,  3.45it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.431      0.152      0.113     0.0507\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/90      3.64G      1.776      1.453      1.396        133        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:48<00:00,  3.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.95it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.304       0.17      0.126     0.0591\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/90      3.59G      1.778      1.444      1.392        219        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.32it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.465      0.161      0.133      0.064\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/90      3.85G      1.747      1.403      1.385        263        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.59it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.484      0.189      0.182     0.0859\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/90      3.42G      1.726      1.386      1.372        300        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.02it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.428      0.217      0.184     0.0898\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/90       3.7G        1.7      1.353      1.358        197        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.239      0.182      0.156     0.0713\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/90      3.15G       1.69      1.338      1.351        172        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.27it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.464      0.178      0.153     0.0729\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      11/90      3.54G      1.683       1.33      1.344        268        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.361      0.183      0.153     0.0742\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      12/90      3.07G      1.678      1.317      1.347        168        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.351      0.168      0.157     0.0749\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      13/90       3.4G      1.668      1.308      1.339        141        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.366      0.196       0.16       0.07\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      14/90      3.67G      1.643      1.271      1.321        277        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.30it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.468      0.188      0.168     0.0793\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      15/90      3.58G       1.66      1.287      1.336        180        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.73it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.423      0.168      0.165       0.08\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      16/90      3.17G      1.639      1.274      1.335        263        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.93it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.414      0.202      0.181     0.0874\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      17/90      3.73G       1.63      1.258      1.319        274        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.57it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.453      0.176      0.178     0.0862\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      18/90      3.58G      1.606      1.234      1.306        238        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.444      0.209      0.206     0.0993\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      19/90       3.2G      1.598      1.224       1.31        194        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.59it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.439      0.236      0.236      0.116\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      20/90       3.2G      1.589      1.211      1.307        401        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.42      0.225      0.197     0.0975\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      21/90      3.19G      1.582      1.192      1.301        199        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.312      0.224      0.197        0.1\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      22/90      3.25G      1.581      1.199      1.303        227        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.431      0.241      0.194     0.0987\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      23/90      3.26G      1.566      1.187      1.304        267        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.94it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.337      0.221      0.209      0.103\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      24/90      3.18G       1.58      1.194      1.301        269        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.13it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.478      0.226      0.197     0.0967\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      25/90      3.75G      1.562      1.166      1.288        192        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.431      0.229      0.212      0.111\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      26/90      3.45G      1.556      1.164      1.289        238        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.51      0.223      0.204     0.0987\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      27/90         3G      1.551      1.157       1.29        242        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.438      0.247      0.214      0.105\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      28/90      2.99G      1.531      1.148      1.281        236        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.11it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.448      0.201      0.193     0.0963\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      29/90         3G      1.549      1.148       1.28        266        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.332      0.219      0.206        0.1\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      30/90      3.17G      1.534      1.131      1.272        324        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.428      0.231      0.215      0.109\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      31/90      3.74G      1.513      1.117      1.265        153        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.28      0.244      0.197      0.101\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      32/90      3.65G        1.5      1.097      1.259        276        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.422      0.229      0.188     0.0953\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      33/90      3.12G      1.505      1.105      1.269        356        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.73it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.549      0.199      0.192     0.0939\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      34/90      3.16G      1.501      1.091      1.256        246        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.59it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.415      0.231      0.197      0.104\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      35/90      3.19G      1.492      1.091       1.27        188        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.98it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.494      0.241      0.231      0.114\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      36/90      3.43G      1.491       1.09      1.255        187        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.00it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.31      0.228      0.207       0.11\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      37/90      3.26G      1.489      1.082      1.258        214        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.393      0.224      0.198      0.101\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      38/90      3.48G      1.469      1.066      1.242        245        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.418      0.241      0.219       0.11\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      39/90      3.53G      1.463      1.063      1.242        143        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.55it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.467      0.237      0.239      0.124\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      40/90      2.96G      1.474      1.066      1.247        218        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.459      0.231       0.22      0.113\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      41/90      3.48G      1.457      1.054       1.24        253        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.425       0.26      0.221      0.107\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      42/90      3.19G      1.451      1.061       1.24        136        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.98it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.452      0.235      0.234      0.123\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      43/90      3.38G      1.462      1.054      1.244        197        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.43      0.216      0.196      0.105\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      44/90      3.47G      1.457      1.051      1.238        156        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.441      0.241      0.218      0.112\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      45/90      2.94G      1.429       1.02      1.228        216        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.531      0.249      0.229      0.119\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      46/90      3.37G       1.43      1.025      1.225        288        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.59it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.402       0.24      0.189      0.098\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      47/90      3.23G      1.427      1.021      1.221        332        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.55it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.15it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.279      0.255      0.217       0.11\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      48/90       3.2G      1.411       1.01      1.216        278        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.57it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.498      0.205      0.202      0.107\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      49/90      3.58G      1.409      0.996      1.214        152        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:48<00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.53it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.451      0.213      0.199      0.106\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      50/90      3.26G      1.419      1.012      1.221        340        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.53it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.255      0.288      0.207      0.107\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      51/90       3.6G      1.399     0.9917      1.209        108        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.56it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.37it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.517      0.236      0.223      0.117\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      52/90      3.11G      1.398     0.9875      1.205        212        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.56it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.419      0.244      0.241       0.12\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      53/90      3.38G      1.391      0.987        1.2        209        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.431      0.247      0.218      0.111\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      54/90      3.18G       1.39     0.9825      1.207        166        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.348      0.247      0.231      0.121\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      55/90      3.74G      1.387     0.9823        1.2        232        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.423      0.264      0.258      0.133\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      56/90      3.23G      1.389     0.9823      1.201        292        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:45<00:00,  3.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.32      0.231      0.217      0.115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      57/90      3.13G      1.364     0.9563      1.189        232        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.12it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.316      0.253      0.228      0.122\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      58/90      3.13G      1.372     0.9588      1.194        371        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.489      0.245      0.244      0.129\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      59/90      3.41G      1.354     0.9502      1.187        305        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.92it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.374      0.243      0.241      0.127\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      60/90      3.54G      1.364     0.9609      1.192        338        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:45<00:00,  3.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.65it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.309      0.251      0.219      0.115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      61/90      3.11G      1.342     0.9392      1.185        289        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.30it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482        0.5       0.25      0.242      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      62/90      3.09G      1.345     0.9339      1.176        278        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:45<00:00,  3.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.32it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.31      0.263      0.244       0.13\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      63/90      2.94G      1.348     0.9426      1.181        280        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:45<00:00,  3.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.78it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.334      0.267      0.226      0.118\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      64/90      3.61G      1.342     0.9286      1.175        188        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.92it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.348      0.234      0.223      0.116\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      65/90      2.87G      1.334     0.9219      1.172        254        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.452      0.249      0.232      0.119\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      66/90      3.24G      1.319     0.9116      1.161        312        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.57it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.328      0.246      0.236      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      67/90      3.15G      1.314     0.9044      1.159        183        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.375      0.235      0.235      0.124\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      68/90      3.46G      1.317     0.9055      1.161        264        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.57it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.04it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.349      0.234      0.235      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      69/90      3.29G      1.311     0.9046      1.162        233        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.53it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.463      0.241      0.229       0.12\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      70/90      2.85G        1.3     0.8951      1.158        191        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.393      0.234      0.246      0.129\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      71/90       3.4G      1.298     0.8871      1.152        208        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.12it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.451      0.238      0.228      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      72/90      3.45G      1.293     0.8921      1.155        219        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.57it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.75it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.307      0.246      0.218      0.115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      73/90      3.33G      1.276     0.8787      1.148        206        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.06it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.261      0.231      0.196      0.108\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      74/90      3.56G      1.271     0.8697      1.141        228        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.59it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.493      0.209       0.22      0.122\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      75/90       3.2G      1.273     0.8671      1.143        222        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.56it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.53it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.465      0.222      0.224      0.118\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      76/90      3.58G      1.265     0.8655      1.141        165        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:46<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.32it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.307      0.253      0.232      0.124\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      77/90      3.28G      1.263      0.863      1.139        209        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.341      0.232      0.211      0.113\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      78/90      2.94G      1.256     0.8536       1.13        261        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.59it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.332      0.229      0.222      0.121\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      79/90      3.81G      1.248     0.8472      1.134        313        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.56it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.292      0.231      0.213      0.116\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      80/90      3.29G      1.234     0.8338      1.128        169        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:47<00:00,  3.59it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.348      0.234      0.231      0.121\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      81/90       2.7G      1.227      0.809      1.126         85        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:48<00:00,  3.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.364      0.229      0.214      0.115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      82/90      2.72G      1.205     0.7864      1.118         88        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  3.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.448      0.217      0.239      0.128\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      83/90      2.67G      1.188     0.7721      1.104        111        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  3.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.358      0.228      0.219      0.119\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      84/90      2.72G      1.189     0.7763      1.107        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  3.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.375       0.23      0.224      0.119\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      85/90      2.67G      1.176      0.765      1.104         69        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  3.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.286       0.24      0.207      0.113\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      86/90      2.65G      1.155     0.7508      1.091        109        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  4.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.346      0.213      0.212      0.115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      87/90      2.65G      1.152      0.749      1.091        130        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:42<00:00,  3.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.326      0.222      0.216      0.117\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      88/90      2.72G      1.147     0.7408      1.087        120        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.03it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.344      0.215      0.224      0.121\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      89/90      2.62G      1.139     0.7381      1.085        131        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:43<00:00,  3.91it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.30it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.33      0.218      0.215      0.118\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      90/90      2.65G      1.137     0.7376      1.083         84        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:41<00:00,  4.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.82it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.328      0.227       0.22      0.119\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n90 epochs completed in 1.272 hours.\nOptimizer stripped from GRD_Object_detection/26Jan2024/weights/last.pt, 6.2MB\nOptimizer stripped from GRD_Object_detection/26Jan2024/weights/best.pt, 6.2MB\n\nValidating GRD_Object_detection/26Jan2024/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.422      0.267      0.258      0.132\nhuman.pedestrian.adult        300         95      0.415       0.41      0.364      0.143\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.258      0.125     0.0604     0.0263\n           vehicle.car        300        393      0.684      0.537      0.613      0.335\n         vehicle.truck        300         63      0.362      0.667      0.439      0.277\nmovable_object.barrier        300         80      0.685     0.0549      0.108     0.0386\n movable_object.debris        300          7          0          0     0.0359      0.013\nmovable_object.pushable_pullable        300        655       0.63      0.374       0.42      0.198\nmovable_object.trafficcone        300        148      0.767      0.236      0.285      0.159\nSpeed: 0.6ms preprocess, 1.3ms inference, 0.0ms loss, 1.1ms postprocess per image\nResults saved to \u001b[1mGRD_Object_detection/26Jan2024\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Fusing after 20 epochs","metadata":{}},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_10epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch10.pt'\nfused_weights_backup = 'epoch_20_fused.pt'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:41:55.872642Z","iopub.execute_input":"2024-01-26T14:41:55.873022Z","iopub.status.idle":"2024-01-26T14:41:58.902680Z","shell.execute_reply.started":"2024-01-26T14:41:55.872996Z","shell.execute_reply":"2024-01-26T14:41:58.901614Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_20_fused.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"model_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name='26Jan2024_20epoch', optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:43:53.448218Z","iopub.execute_input":"2024-01-26T14:43:53.449035Z","iopub.status.idle":"2024-01-26T14:53:20.209580Z","shell.execute_reply.started":"2024-01-26T14:43:53.448996Z","shell.execute_reply":"2024-01-26T14:53:20.208581Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_10epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_20epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_20epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_20epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1034.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 1003.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\nPlotting labels to RGB_Object_detection/26Jan2024_20epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_20epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.88G      1.509      1.083      1.271         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:48<00:00,  3.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.457      0.271      0.257      0.122\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.68G      1.584      1.175      1.311         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.49      0.245      0.242      0.116\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G      1.606      1.187      1.329         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.04it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.425      0.245      0.238      0.112\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.73G      1.592      1.186      1.325         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.00it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.418      0.275      0.232      0.107\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.76G      1.571      1.147      1.315         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.10it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.546      0.198       0.22      0.113\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.75G      1.563      1.142      1.306         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.429      0.297      0.271      0.136\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.76G      1.528      1.099      1.289         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.84it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.481      0.299      0.264      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.76G      1.497       1.06      1.267         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.43      0.316      0.285      0.141\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.71G      1.454      1.019      1.245         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.50it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.605      0.284      0.278      0.138\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.69G      1.427     0.9887      1.224         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.03it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.482      0.324      0.295      0.149\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.131 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_20epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_20epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_20epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.48      0.325      0.295      0.149\nhuman.pedestrian.adult        300         95      0.638      0.379      0.366      0.188\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.367       0.04     0.0376     0.0045\n           vehicle.car        300        393      0.672      0.584      0.639      0.352\n         vehicle.truck        300         63      0.327      0.698      0.403      0.266\nmovable_object.barrier        300         80      0.109        0.3      0.182     0.0686\n movable_object.debris        300          7          1          0     0.0485     0.0145\nmovable_object.pushable_pullable        300        655      0.606      0.472      0.507      0.245\nmovable_object.trafficcone        300        148      0.601      0.448      0.473      0.203\nSpeed: 0.6ms preprocess, 1.3ms inference, 0.0ms loss, 1.0ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_20epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_20epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch20.pt'\nfused_weights_backup = 'epoch_30_fused.pt'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:56:20.250040Z","iopub.execute_input":"2024-01-26T14:56:20.250414Z","iopub.status.idle":"2024-01-26T14:56:23.223091Z","shell.execute_reply.started":"2024-01-26T14:56:20.250387Z","shell.execute_reply":"2024-01-26T14:56:23.222115Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_30_fused.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"model_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name='26Jan2024_30epoch', optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:57:15.695162Z","iopub.execute_input":"2024-01-26T14:57:15.695545Z","iopub.status.idle":"2024-01-26T15:06:19.911113Z","shell.execute_reply.started":"2024-01-26T14:57:15.695516Z","shell.execute_reply":"2024-01-26T15:06:19.910096Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_20epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_30epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_30epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_30epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1058.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 838.32it/s] ","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/26Jan2024_30epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_30epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.89G      1.449      1.015      1.242         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:48<00:00,  3.53it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.90it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.501      0.267      0.269       0.13\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10       2.7G      1.512      1.084      1.273         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.27it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.504      0.268      0.262      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G      1.541      1.105      1.297         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.365      0.232      0.194     0.0956\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.74G      1.542      1.124      1.297         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.571       0.25      0.254      0.127\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.77G      1.511      1.086      1.282         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.26it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.46      0.245      0.241      0.117\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.75G      1.504       1.07      1.279         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.02it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.424      0.272      0.282      0.139\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.76G      1.477      1.031      1.262         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.91it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.411      0.244      0.233       0.11\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.77G      1.441      1.002      1.238         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.495      0.266      0.266      0.129\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.72G      1.403     0.9609      1.216         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.448      0.276      0.285      0.144\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.69G      1.379     0.9334      1.203         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.401      0.295      0.285      0.145\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.143 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_30epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_30epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_30epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482        0.4      0.296      0.285      0.145\nhuman.pedestrian.adult        300         95      0.729      0.379      0.404      0.207\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.137       0.04     0.0442     0.0203\n           vehicle.car        300        393      0.729      0.494      0.611      0.337\n         vehicle.truck        300         63      0.292      0.635      0.294      0.193\nmovable_object.barrier        300         80      0.188       0.25       0.19     0.0741\n movable_object.debris        300          7          0          0     0.0049     0.0014\nmovable_object.pushable_pullable        300        655      0.724      0.466      0.554       0.27\nmovable_object.trafficcone        300        148      0.798      0.401      0.462      0.206\nSpeed: 0.8ms preprocess, 1.5ms inference, 0.0ms loss, 1.7ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_30epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_30epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch30.pt'\nfused_weights_backup = 'epoch_40_fused.pt'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:08:39.879596Z","iopub.execute_input":"2024-01-26T15:08:39.880427Z","iopub.status.idle":"2024-01-26T15:08:42.911973Z","shell.execute_reply.started":"2024-01-26T15:08:39.880391Z","shell.execute_reply":"2024-01-26T15:08:42.910896Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_40_fused.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"model_1 = YOLO(rgb_last_weights_file)\nfolder_name = '26Jan2024_40epoch'\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:10:20.099105Z","iopub.execute_input":"2024-01-26T15:10:20.100039Z","iopub.status.idle":"2024-01-26T15:19:10.023564Z","shell.execute_reply.started":"2024-01-26T15:10:20.100006Z","shell.execute_reply":"2024-01-26T15:19:10.022570Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_30epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_40epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_40epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_40epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:40<00:00, 66.13it/s]  ","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 772.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\nPlotting labels to RGB_Object_detection/26Jan2024_40epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_40epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.84G      1.402     0.9646      1.218         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:47<00:00,  3.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.16it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.402      0.296      0.282      0.142\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.68G      1.459      1.022      1.247         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.27it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.505       0.25      0.243      0.118\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G      1.486      1.052      1.266         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.94it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.364       0.28      0.252      0.117\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.73G      1.481      1.053      1.264         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.02it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.326      0.255      0.243      0.119\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.73G       1.47      1.036      1.258         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.398      0.262      0.267      0.132\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.75G      1.457      1.014      1.251         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.06it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.37       0.29      0.288      0.142\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.72G      1.433     0.9897      1.239         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.55it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.329      0.295       0.26      0.123\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.73G      1.405     0.9594      1.219         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.94it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.491      0.272      0.262      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.71G      1.376     0.9324      1.203         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.42it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.426      0.307      0.309      0.159\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.68G      1.348     0.9028      1.186         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.404      0.287      0.285      0.151\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.128 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_40epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_40epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_40epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.427      0.307      0.309       0.16\nhuman.pedestrian.adult        300         95      0.556      0.369      0.387      0.192\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25       0.36       0.12      0.103      0.032\n           vehicle.car        300        393       0.71      0.585      0.658      0.365\n         vehicle.truck        300         63      0.299      0.635      0.454      0.312\nmovable_object.barrier        300         80      0.363      0.212      0.193     0.0752\n movable_object.debris        300          7          0          0   0.000564   0.000169\nmovable_object.pushable_pullable        300        655      0.688      0.449      0.519      0.264\nmovable_object.trafficcone        300        148      0.865      0.389      0.468      0.199\nSpeed: 1.1ms preprocess, 1.7ms inference, 0.0ms loss, 1.5ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_40epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_40epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch40.pt'\nfused_weights_backup = 'epoch_50_fused.pt'\nfolder_name = '26Jan2024_50epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:21:58.877775Z","iopub.execute_input":"2024-01-26T15:21:58.878188Z","iopub.status.idle":"2024-01-26T15:30:56.250194Z","shell.execute_reply.started":"2024-01-26T15:21:58.878156Z","shell.execute_reply":"2024-01-26T15:30:56.249071Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_50_fused.pt\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_40epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_50epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_50epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_50epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1092.14it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 959.69it/s] ","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/26Jan2024_50epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_50epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.85G      1.371     0.9354      1.202         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:47<00:00,  3.57it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.368      0.261      0.251      0.121\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.69G      1.418     0.9867      1.229         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.312      0.274      0.225      0.108\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G       1.44      1.007      1.244         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.16it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482        0.3      0.229      0.211      0.103\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.73G       1.46      1.025      1.251         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.523      0.253      0.245      0.119\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.77G      1.443      1.001      1.244         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.87it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.531      0.249      0.248      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.75G      1.438      1.004      1.241         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.39it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.469      0.279      0.289      0.146\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.76G       1.41     0.9664      1.225         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.475      0.287      0.266      0.132\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.77G      1.375     0.9354      1.205         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.78it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.531      0.256      0.252      0.122\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.71G       1.34     0.8992      1.187         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.42      0.303      0.303      0.155\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.69G      1.328     0.8849      1.175         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.402      0.282      0.281      0.147\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.129 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_50epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_50epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_50epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.421      0.303      0.303      0.155\nhuman.pedestrian.adult        300         95      0.649        0.4      0.445      0.221\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.275       0.12     0.0993     0.0297\n           vehicle.car        300        393      0.717       0.59      0.654      0.366\n         vehicle.truck        300         63      0.339      0.587      0.333       0.22\nmovable_object.barrier        300         80      0.312      0.175      0.171     0.0708\n movable_object.debris        300          7          0          0     0.0017   0.000272\nmovable_object.pushable_pullable        300        655        0.7      0.463      0.538      0.263\nmovable_object.trafficcone        300        148      0.797      0.392      0.484      0.224\nSpeed: 0.8ms preprocess, 3.9ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_50epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_50epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch50.pt'\nfused_weights_backup = 'epoch_60_fused.pt'\nfolder_name = '26Jan2024_60epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:32:13.177313Z","iopub.execute_input":"2024-01-26T15:32:13.178178Z","iopub.status.idle":"2024-01-26T15:41:06.489658Z","shell.execute_reply.started":"2024-01-26T15:32:13.178144Z","shell.execute_reply":"2024-01-26T15:41:06.488651Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_60_fused.pt\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_50epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_60epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_60epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_60epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1077.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 769.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\nPlotting labels to RGB_Object_detection/26Jan2024_60epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_60epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.89G      1.341     0.9058      1.188         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:47<00:00,  3.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.97it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.38      0.309      0.295      0.147\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.71G      1.386     0.9488      1.208         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.92it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.438      0.236       0.23      0.111\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10       2.7G       1.41     0.9764      1.226         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.353      0.266      0.235      0.115\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.74G      1.424     0.9959      1.237         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.357      0.276      0.256      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.77G      1.418     0.9729      1.232         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.527      0.269      0.275      0.141\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.76G      1.404      0.969      1.225         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.364      0.312      0.281      0.142\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.77G      1.372     0.9319      1.208         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.85it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.422      0.309      0.304      0.155\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.78G      1.349      0.906      1.191         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.04it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.473       0.28      0.265      0.135\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.72G      1.323     0.8805      1.177         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.27it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.452      0.275       0.29      0.149\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10       2.7G      1.303     0.8617      1.163         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.397      0.298      0.296      0.156\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.139 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_60epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_60epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_60epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.421      0.309      0.304      0.155\nhuman.pedestrian.adult        300         95      0.609      0.516      0.499       0.25\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.454      0.101     0.0982     0.0443\n           vehicle.car        300        393      0.638      0.519      0.592      0.328\n         vehicle.truck        300         63      0.292      0.651      0.386      0.214\nmovable_object.barrier        300         80      0.283        0.2      0.187     0.0847\n movable_object.debris        300          7          0          0          0          0\nmovable_object.pushable_pullable        300        655      0.705      0.421      0.507      0.252\nmovable_object.trafficcone        300        148      0.811      0.377      0.462      0.226\nSpeed: 0.5ms preprocess, 1.2ms inference, 0.0ms loss, 1.1ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_60epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_60epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch60.pt'\nfused_weights_backup = 'epoch_70_fused.pt'\nfolder_name = '26Jan2024_70epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:42:25.328330Z","iopub.execute_input":"2024-01-26T15:42:25.328740Z","iopub.status.idle":"2024-01-26T15:51:20.931607Z","shell.execute_reply.started":"2024-01-26T15:42:25.328708Z","shell.execute_reply":"2024-01-26T15:51:20.930415Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_70_fused.pt\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_60epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_70epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_70epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_70epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1048.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 1032.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\nPlotting labels to RGB_Object_detection/26Jan2024_70epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_70epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.85G      1.322       0.89      1.178         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:47<00:00,  3.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.523      0.268      0.276      0.131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.68G      1.361     0.9216      1.195         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.38it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.476      0.268      0.257      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G      1.381     0.9527       1.21         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.367       0.28      0.251      0.126\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.73G      1.395     0.9715       1.22         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.32      0.271      0.249      0.124\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.73G      1.393     0.9516      1.219         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.15it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.353      0.273      0.254      0.131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.75G      1.381     0.9434      1.213         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.401      0.298      0.304      0.156\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.73G      1.363     0.9186      1.198         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.39it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.433      0.319      0.312      0.156\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.73G      1.329     0.8852       1.18         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.30it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.509      0.281      0.274      0.137\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.71G      1.301     0.8612      1.164         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.561      0.255      0.279      0.146\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.68G      1.286     0.8439      1.155         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.89it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.428      0.276      0.299       0.16\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.129 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_70epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_70epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_70epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.428      0.271      0.299       0.16\nhuman.pedestrian.adult        300         95      0.647      0.368      0.392      0.207\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.116       0.08     0.0368     0.0107\n           vehicle.car        300        393      0.707      0.478      0.599      0.351\n         vehicle.truck        300         63      0.306      0.651      0.508      0.357\nmovable_object.barrier        300         80       0.29       0.26      0.184     0.0695\n movable_object.debris        300          7          0          0    0.00101   0.000101\nmovable_object.pushable_pullable        300        655      0.875       0.33      0.511       0.26\nmovable_object.trafficcone        300        148       0.91      0.273      0.456      0.184\nSpeed: 0.6ms preprocess, 1.2ms inference, 0.0ms loss, 1.0ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_70epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch70.pt'\nfused_weights_backup = 'epoch_80_fused.pt'\nfolder_name = '26Jan2024_80epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:53:00.659566Z","iopub.execute_input":"2024-01-26T15:53:00.660677Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_80_fused.pt\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_80epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_80epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_80epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1125.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 1009.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\nPlotting labels to RGB_Object_detection/26Jan2024_80epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_80epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.91G      1.302     0.8607      1.168         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:46<00:00,  3.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.36      0.282      0.268      0.132\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.72G      1.338     0.9068      1.182         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:40<00:00,  4.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.06it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.534      0.283        0.3       0.15\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.71G      1.363     0.9298      1.198         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.407      0.261      0.261      0.123\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.75G      1.382     0.9474      1.212         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:41<00:00,  4.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.377      0.245      0.245      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.76G      1.374     0.9338      1.205        144        640:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 84/170 [00:20<00:18,  4.72it/s]","output_type":"stream"}]},{"cell_type":"code","source":"rgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch70.pt'\nfused_weights_backup = 'epoch_80_fused.pt'\nfolder_name = '26Jan2024_80epoch'\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T16:06:01.799241Z","iopub.execute_input":"2024-01-26T16:06:01.800311Z","iopub.status.idle":"2024-01-26T16:15:11.517128Z","shell.execute_reply.started":"2024-01-26T16:06:01.800270Z","shell.execute_reply":"2024-01-26T16:15:11.515926Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_80epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_80epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_80epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1054.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 916.49it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/26Jan2024_80epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_80epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.85G      1.302     0.8607      1.168         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:49<00:00,  3.46it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.98it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.36      0.282      0.268      0.132\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.68G      1.338     0.9068      1.182         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:43<00:00,  3.94it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.534      0.283        0.3       0.15\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.69G      1.363     0.9298      1.198         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.407      0.261      0.261      0.123\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.73G      1.382     0.9474      1.212         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:43<00:00,  3.92it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.65it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.377      0.245      0.245      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.77G      1.362      0.926      1.202         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:43<00:00,  3.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.387      0.274      0.266      0.135\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.75G       1.36     0.9238      1.205         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:43<00:00,  3.94it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.461      0.284      0.286      0.137\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.76G      1.334     0.8989      1.187         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.37it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.486      0.312      0.294      0.145\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.76G      1.309     0.8712       1.17         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.89it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.389      0.277      0.263       0.13\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.71G      1.285     0.8484      1.158         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.514      0.291      0.296      0.157\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.69G      1.271     0.8293      1.146         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.04it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.387      0.295      0.292      0.156\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.134 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_80epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_80epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_80epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.513      0.291      0.296      0.157\nhuman.pedestrian.adult        300         95      0.639      0.358      0.363      0.192\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0807       0.04     0.0196     0.0046\n           vehicle.car        300        393      0.763      0.562      0.654      0.364\n         vehicle.truck        300         63      0.298      0.619      0.424      0.293\nmovable_object.barrier        300         80      0.297       0.25       0.19     0.0748\n movable_object.debris        300          7          1          0    0.00126   0.000504\nmovable_object.pushable_pullable        300        655      0.757        0.4      0.527      0.269\nmovable_object.trafficcone        300        148      0.784      0.385      0.485      0.218\nSpeed: 0.7ms preprocess, 1.6ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_80epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_80epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch80.pt'\nfused_weights_backup = 'epoch_90_fused.pt'\nfolder_name = '26Jan2024_90epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T16:17:00.449447Z","iopub.execute_input":"2024-01-26T16:17:00.450151Z","iopub.status.idle":"2024-01-26T16:26:11.956045Z","shell.execute_reply.started":"2024-01-26T16:17:00.450114Z","shell.execute_reply":"2024-01-26T16:26:11.955066Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_90_fused.pt\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_80epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_90epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_90epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_90epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1132.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 921.58it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/26Jan2024_90epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_90epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.89G      1.287     0.8512       1.16         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:49<00:00,  3.45it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.374      0.279      0.268      0.131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.72G      1.322     0.8828      1.173         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:43<00:00,  3.92it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.27it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.486      0.275      0.274      0.137\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.71G      1.351     0.9159      1.193         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.41      0.234       0.24       0.11\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.75G      1.365     0.9312      1.204         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:43<00:00,  3.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.371      0.262      0.271      0.135\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.78G      1.352     0.9158      1.199         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.26it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.418       0.29      0.304      0.154\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.77G       1.34     0.9058      1.193         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.409      0.286      0.298      0.153\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.78G      1.319     0.8735      1.179         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.13it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.397      0.281      0.289      0.152\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.79G      1.292     0.8586      1.162         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.505      0.297      0.288      0.146\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      2.73G      1.268     0.8322      1.148         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.389      0.311      0.293      0.152\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.71G      1.258     0.8171       1.14         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.391      0.295      0.291      0.155\n\n10 epochs completed in 0.139 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_90epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_90epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_90epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482       0.42       0.29      0.304      0.154\nhuman.pedestrian.adult        300         95      0.667      0.526      0.538      0.274\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25     0.0648       0.08      0.063     0.0139\n           vehicle.car        300        393      0.758      0.435      0.602      0.327\n         vehicle.truck        300         63      0.327      0.667        0.4      0.269\nmovable_object.barrier        300         80      0.513      0.132      0.177     0.0655\n movable_object.debris        300          7          0          0   0.000993   0.000274\nmovable_object.pushable_pullable        300        655      0.584       0.46       0.51      0.253\nmovable_object.trafficcone        300        148      0.868       0.31      0.449      0.185\nSpeed: 0.7ms preprocess, 2.2ms inference, 0.0ms loss, 3.9ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_90epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nrgb_last_weights_file = '/kaggle/working/RGB_Object_detection/26Jan2024_90epoch/weights/last.pt'\ngrd_last_weights_file =  '/kaggle/working/GRD_Object_detection/26Jan2024/weights/last.pt'\nfused_weights_backup = 'epoch_100_fused.pt'\nfolder_name = '26Jan2024_100epoch'\n\n# Load the first model\nmodel1_dict = torch.load(rgb_last_weights_file, map_location=torch.device(0))\n\n# Load the second model\nmodel2_dict = torch.load(grd_last_weights_file, map_location=torch.device(0))\n\n# Update model1 with the weights from model2\nfor key in model1_dict['model'].state_dict().keys():\n    model1_dict['model'].state_dict()[key] = (model1_dict['model'].state_dict()[key] + model2_dict['model'].state_dict()[key]) / 2.0\n\n# Save the updated model1\ntorch.save(model1_dict, rgb_last_weights_file)\ntorch.save(model1_dict, fused_weights_backup)\nprint(\"Fusion of weights done: \", fused_weights_backup)\n\nmodel_1 = YOLO(rgb_last_weights_file)\nresults_1 = model_1.train(data='/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml', epochs=10, save_period=2, device=0, project='RGB_Object_detection', name=folder_name, optimizer = 'Adam')  # train the model","metadata":{"execution":{"iopub.status.busy":"2024-01-26T16:28:19.826236Z","iopub.execute_input":"2024-01-26T16:28:19.827046Z","iopub.status.idle":"2024-01-26T16:37:32.338385Z","shell.execute_reply.started":"2024-01-26T16:28:19.827008Z","shell.execute_reply":"2024-01-26T16:37:32.337199Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Fusion of weights done:  epoch_100_fused.pt\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/working/RGB_Object_detection/26Jan2024_90epoch/weights/last.pt, data=/kaggle/input/rgb-yaml-file/rgb_dataset_kaggle_updated.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=2, cache=False, device=0, workers=8, project=RGB_Object_detection, name=26Jan2024_100epoch, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=RGB_Object_detection/26Jan2024_100epoch\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    755797  ultralytics.nn.modules.head.Detect           [23, [64, 128, 256]]          \nModel summary: 225 layers, 3015333 parameters, 3015317 gradients, 8.2 GFLOPs\n\nTransferred 355/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RGB_Object_detection/26Jan2024_100epoch', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb/labels... 2707 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2708/2708 [00:02<00:00, 1042.85it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/train_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb/labels... 300 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 912.04it/s] ","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/dataset-kaggle-nuscenes/dataset_kaggle_nuscenes_vishnu/dataset/valid_rgb is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to RGB_Object_detection/26Jan2024_100epoch/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mRGB_Object_detection/26Jan2024_100epoch\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      2.84G       1.27     0.8414      1.153         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:49<00:00,  3.45it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.457      0.264      0.241      0.111\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      2.67G      1.304     0.8726      1.169         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:43<00:00,  3.93it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.59it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.466      0.287      0.282      0.137\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      2.67G      1.325     0.8955      1.185         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.302      0.223      0.185     0.0837\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      2.72G      1.343     0.9163      1.196         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.302      0.281       0.27      0.137\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      2.71G      1.339     0.9025      1.189         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.06it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.368      0.279      0.266      0.134\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      2.73G      1.333     0.8986      1.189         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.50it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.541      0.279      0.299      0.145\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      2.71G      1.301     0.8676      1.171         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.10it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.321      0.283       0.25      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      2.72G      1.281     0.8456      1.159         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.18it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.468      0.284       0.25      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10       2.7G      1.255     0.8222      1.143         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  3.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.406      0.311       0.31      0.163\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      2.67G      1.244     0.8075      1.136         79        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:42<00:00,  4.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.05it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.484      0.298      0.318      0.171\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.133 hours.\nOptimizer stripped from RGB_Object_detection/26Jan2024_100epoch/weights/last.pt, 6.2MB\nOptimizer stripped from RGB_Object_detection/26Jan2024_100epoch/weights/best.pt, 6.2MB\n\nValidating RGB_Object_detection/26Jan2024_100epoch/weights/best.pt...\nUltralytics YOLOv8.1.6 ðŸš€ Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 3010133 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        300       1482      0.479      0.299      0.318      0.171\nhuman.pedestrian.adult        300         95      0.832      0.468      0.522      0.286\nhuman.pedestrian.police_officer        300         16          0          0          0          0\n       vehicle.bicycle        300         25      0.394       0.16     0.0838     0.0311\n           vehicle.car        300        393      0.826      0.471      0.612       0.36\n         vehicle.truck        300         63      0.299      0.651      0.495      0.354\nmovable_object.barrier        300         80      0.272       0.25      0.181     0.0643\n movable_object.debris        300          7          0          0    0.00065    0.00013\nmovable_object.pushable_pullable        300        655      0.869      0.385      0.546      0.269\nmovable_object.trafficcone        300        148      0.818      0.304      0.424      0.176\nSpeed: 0.6ms preprocess, 1.3ms inference, 0.0ms loss, 0.9ms postprocess per image\nResults saved to \u001b[1mRGB_Object_detection/26Jan2024_100epoch\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r knowledge_distillation_26Jan2024.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-01-26T16:55:36.572510Z","iopub.execute_input":"2024-01-26T16:55:36.573057Z","iopub.status.idle":"2024-01-26T16:58:35.984288Z","shell.execute_reply.started":"2024-01-26T16:55:36.573019Z","shell.execute_reply":"2024-01-26T16:58:35.982922Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/epoch_10_fused.pt (deflated 8%)\n  adding: kaggle/working/epoch_40_fused.pt (deflated 35%)\n  adding: kaggle/working/epoch_80_fused.pt (deflated 40%)\n  adding: kaggle/working/RGB_Object_detection/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/confusion_matrix_normalized.png (deflated 16%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/results.csv (deflated 82%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/P_curve.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/events.out.tfevents.1706283136.039e61d1f8cb.26.10 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/weights/epoch4.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/weights/epoch2.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/weights/best.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/weights/last.pt (deflated 39%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/weights/epoch6.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/weights/epoch8.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_60epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/confusion_matrix.png (deflated 16%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/val_batch2_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/P_curve.png (deflated 7%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model2/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/confusion_matrix.png (deflated 16%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/events.out.tfevents.1706280233.039e61d1f8cb.26.6 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/confusion_matrix_normalized.png (deflated 16%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/results.csv (deflated 82%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/P_curve.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/weights/epoch4.pt (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/weights/epoch2.pt (deflated 11%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/weights/best.pt (deflated 34%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/weights/last.pt (deflated 34%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/weights/epoch6.pt (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/weights/epoch8.pt (deflated 18%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_20epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/events.out.tfevents.1706281036.039e61d1f8cb.26.7 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/results.csv (deflated 82%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/P_curve.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/weights/epoch4.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/weights/epoch2.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/weights/best.pt (deflated 35%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/weights/last.pt (deflated 35%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/weights/epoch6.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/weights/epoch8.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_30epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/confusion_matrix.png (deflated 16%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/results.csv (deflated 82%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/P_curve.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/epoch4.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/epoch2.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/best.pt (deflated 40%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/last.pt (deflated 40%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/epoch6.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/weights/epoch8.pt (deflated 38%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/events.out.tfevents.1706283748.039e61d1f8cb.26.11 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_70epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/results.csv (deflated 81%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/P_curve.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/weights/epoch4.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/weights/epoch2.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/weights/best.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/weights/last.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/weights/epoch6.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/weights/epoch8.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/events.out.tfevents.1706282522.039e61d1f8cb.26.9 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_50epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/args.yaml (deflated 51%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/results.png (deflated 7%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/results.csv (deflated 82%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/P_curve.png (deflated 7%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/events.out.tfevents.1706271509.039e61d1f8cb.26.0 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/weights/epoch4.pt (deflated 8%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/weights/epoch2.pt (deflated 8%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/weights/best.pt (deflated 8%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/weights/last.pt (deflated 8%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/weights/epoch6.pt (deflated 8%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/weights/epoch8.pt (deflated 8%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/confusion_matrix.png (deflated 16%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/events.out.tfevents.1706281820.039e61d1f8cb.26.8 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/results.csv (deflated 82%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/P_curve.png (deflated 7%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/weights/epoch4.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/weights/epoch2.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/weights/best.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/weights/last.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/weights/epoch6.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/weights/epoch8.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_40epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/events.out.tfevents.1706286503.039e61d1f8cb.26.15 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/results.csv (deflated 82%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/P_curve.png (deflated 7%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/F1_curve.png (deflated 8%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/weights/epoch4.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/weights/epoch2.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/weights/best.pt (deflated 40%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/weights/last.pt (deflated 40%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/weights/epoch6.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/weights/epoch8.pt (deflated 38%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_100epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model3/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model3/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model3/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model3/events.out.tfevents.1706273550.039e61d1f8cb.26.2 (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model3/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_model3/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/confusion_matrix.png (deflated 16%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/events.out.tfevents.1706285844.039e61d1f8cb.26.14 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/val_batch2_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/results.csv (deflated 82%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/P_curve.png (deflated 7%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/weights/epoch4.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/weights/epoch2.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/weights/best.pt (deflated 35%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/weights/last.pt (deflated 40%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/weights/epoch6.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/weights/epoch8.pt (deflated 38%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_90epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/events.out.tfevents.1706285162.039e61d1f8cb.26.13 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/results.csv (deflated 81%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/P_curve.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/weights/epoch4.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/weights/epoch2.pt (deflated 36%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/weights/best.pt (deflated 38%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/weights/last.pt (deflated 39%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/weights/epoch6.pt (deflated 37%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/weights/epoch8.pt (deflated 38%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_80epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/labels.jpg (deflated 20%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/val_batch2_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/R_curve.png (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/args.yaml (deflated 52%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/val_batch1_pred.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/train_batch0.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/val_batch1_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/val_batch0_labels.jpg (deflated 5%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/confusion_matrix_normalized.png (deflated 15%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/events.out.tfevents.1706273977.039e61d1f8cb.26.4 (deflated 93%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/results.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/val_batch2_pred.jpg (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/results.csv (deflated 82%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/train_batch1.jpg (deflated 12%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/P_curve.png (deflated 6%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/F1_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/train_batch2.jpg (deflated 10%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/PR_curve.png (deflated 9%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/weights/ (stored 0%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/weights/epoch4.pt (deflated 7%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/weights/epoch2.pt (deflated 7%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/weights/best.pt (deflated 11%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/weights/last.pt (deflated 11%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/weights/epoch6.pt (deflated 8%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/weights/epoch8.pt (deflated 8%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/RGB_Object_detection/26Jan2024_10epoch/val_batch0_pred.jpg (deflated 5%)\n  adding: kaggle/working/epoch_30_fused.pt (deflated 34%)\n  adding: kaggle/working/epoch_20_fused.pt (deflated 12%)\n  adding: kaggle/working/yolov8n.pt (deflated 9%)\n  adding: kaggle/working/epoch_70_fused.pt (deflated 39%)\n  adding: kaggle/working/epoch_60_fused.pt (deflated 38%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/datasets/ (stored 0%)\n  adding: kaggle/working/datasets/coco8/ (stored 0%)\n  adding: kaggle/working/datasets/coco8/images/ (stored 0%)\n  adding: kaggle/working/datasets/coco8/images/val/ (stored 0%)\n  adding: kaggle/working/datasets/coco8/images/val/000000000049.jpg (deflated 1%)\n  adding: kaggle/working/datasets/coco8/images/val/000000000036.jpg (deflated 0%)\n  adding: kaggle/working/datasets/coco8/images/val/000000000061.jpg (deflated 0%)\n  adding: kaggle/working/datasets/coco8/images/val/000000000042.jpg (deflated 0%)\n  adding: kaggle/working/datasets/coco8/images/train/ (stored 0%)\n  adding: kaggle/working/datasets/coco8/images/train/000000000034.jpg (deflated 0%)\n  adding: kaggle/working/datasets/coco8/images/train/000000000025.jpg (deflated 0%)\n  adding: kaggle/working/datasets/coco8/images/train/000000000030.jpg (deflated 0%)\n  adding: kaggle/working/datasets/coco8/images/train/000000000009.jpg (deflated 0%)\n  adding: kaggle/working/datasets/coco8/labels/ (stored 0%)\n  adding: kaggle/working/datasets/coco8/labels/val.cache (deflated 33%)\n  adding: kaggle/working/datasets/coco8/labels/train.cache (deflated 35%)\n  adding: kaggle/working/datasets/coco8/labels/val/ (stored 0%)\n  adding: kaggle/working/datasets/coco8/labels/val/000000000049.txt (deflated 52%)\n  adding: kaggle/working/datasets/coco8/labels/val/000000000036.txt (deflated 27%)\n  adding: kaggle/working/datasets/coco8/labels/val/000000000061.txt (deflated 47%)\n  adding: kaggle/working/datasets/coco8/labels/val/000000000042.txt (deflated 9%)\n  adding: kaggle/working/datasets/coco8/labels/train/ (stored 0%)\n  adding: kaggle/working/datasets/coco8/labels/train/000000000034.txt (deflated 10%)\n  adding: kaggle/working/datasets/coco8/labels/train/000000000030.txt (deflated 28%)\n  adding: kaggle/working/datasets/coco8/labels/train/000000000025.txt (deflated 27%)\n  adding: kaggle/working/datasets/coco8/labels/train/000000000009.txt (deflated 52%)\n  adding: kaggle/working/datasets/coco8/README.md (deflated 49%)\n  adding: kaggle/working/datasets/coco8/LICENSE (deflated 66%)\n  adding: kaggle/working/epoch_90_fused.pt (deflated 40%)\n  adding: kaggle/working/epoch_50_fused.pt (deflated 36%)\n  adding: kaggle/working/epoch_100_fused.pt (deflated 40%)\n  adding: kaggle/working/GRD_Object_detection/ (stored 0%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/ (stored 0%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/val_batch2_labels.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/R_curve.png (deflated 11%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/val_batch1_pred.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/val_batch1_labels.jpg (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/val_batch0_labels.jpg (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/confusion_matrix_normalized.png (deflated 16%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/val_batch2_pred.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/P_curve.png (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/F1_curve.png (deflated 11%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/PR_curve.png (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model2/val_batch0_pred.jpg (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/ (stored 0%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/labels.jpg (deflated 20%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/events.out.tfevents.1706272162.039e61d1f8cb.26.1 (deflated 93%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/val_batch2_labels.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/R_curve.png (deflated 11%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/args.yaml (deflated 52%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/val_batch1_pred.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/train_batch0.jpg (deflated 17%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/val_batch1_labels.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/val_batch0_labels.jpg (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/confusion_matrix_normalized.png (deflated 16%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/results.png (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/val_batch2_pred.jpg (deflated 11%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/results.csv (deflated 81%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/train_batch1.jpg (deflated 18%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/P_curve.png (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/F1_curve.png (deflated 11%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/train_batch2.jpg (deflated 15%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/PR_curve.png (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/weights/ (stored 0%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/weights/epoch4.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/weights/epoch2.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/weights/best.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/weights/last.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/weights/epoch6.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/weights/epoch8.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024_model/val_batch0_pred.jpg (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/ (stored 0%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/labels.jpg (deflated 20%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/val_batch2_labels.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/R_curve.png (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/args.yaml (deflated 52%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/events.out.tfevents.1706275004.039e61d1f8cb.26.5 (deflated 90%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/val_batch1_pred.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/confusion_matrix.png (deflated 17%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/train_batch13521.jpg (deflated 19%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/train_batch0.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/val_batch1_labels.jpg (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/val_batch0_labels.jpg (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/confusion_matrix_normalized.png (deflated 16%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/results.png (deflated 6%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/train_batch13522.jpg (deflated 17%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/val_batch2_pred.jpg (deflated 11%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/results.csv (deflated 86%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/train_batch1.jpg (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/P_curve.png (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/F1_curve.png (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/train_batch2.jpg (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/train_batch13520.jpg (deflated 19%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/PR_curve.png (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/ (stored 0%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch48.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch12.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch4.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch58.pt (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch50.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch10.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch40.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch2.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch64.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch56.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch14.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch44.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch34.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch78.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch54.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch76.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch52.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch72.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch66.pt (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch30.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch74.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch38.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch80.pt (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch82.pt (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch84.pt (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/best.pt (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch26.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch88.pt (deflated 10%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/last.pt (deflated 18%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch20.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch22.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch16.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch60.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch6.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch8.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch42.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch18.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch70.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch24.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch46.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch28.pt (deflated 6%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch68.pt (deflated 8%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch62.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch86.pt (deflated 9%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch36.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/weights/epoch32.pt (deflated 7%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/labels_correlogram.jpg (deflated 32%)\n  adding: kaggle/working/GRD_Object_detection/26Jan2024/val_batch0_pred.jpg (deflated 9%)\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/knowledge_distillation_26Jan2024.zip","metadata":{"execution":{"iopub.status.busy":"2024-01-26T17:03:13.731116Z","iopub.execute_input":"2024-01-26T17:03:13.731992Z","iopub.status.idle":"2024-01-26T17:03:14.793831Z","shell.execute_reply.started":"2024-01-26T17:03:13.731951Z","shell.execute_reply":"2024-01-26T17:03:14.792418Z"},"trusted":true},"execution_count":33,"outputs":[]}]}